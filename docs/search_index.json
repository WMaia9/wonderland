[["index.html", "Data Science Portfolio A Showcase of my Projects and my abilities Preface", " Data Science Portfolio A Showcase of my Projects and my abilities Wesley Maia 2021-05-11 Preface In this portfolio, you will find case studies covering the area of Data Science. "],["introduction.html", "Introduction Case studies", " Introduction The demand for skilled data science practitioners in industry, academia, and government is rapidly growing. This portfolio introduces concepts and skills that can help it tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression, and machine learning. Case studies Throughout the portfolio, we use motivating case studies. In each case study, we try to realistically mimic a data scientists experience. For each of the concepts covered, we start by asking specific questions and answer these through data analysis. We learn the concepts as a means to answer the questions. Case studies included in the portfolio are: Case Study Concept GDP And Energy Supply Data Cleaning and Data Visualization Medical Insurance Costs Data Visualization and Linear Regression Quantium Retail Strategy And Analytics Statistical Inference Ok Cupid Descriptive statistics and Machine Learning Real Estate São Paulo Rent Prediction Complete Data Science Problem All codes and projects in this portfolio can be found on my Github page: GitHub "],["author.html", "About Me", " About Me I am graduated in Science Physics, at this moment am taking an MBA course in Data Science and Analytics and working with projects in the area. I am passionate about technology and science. At all times I am always looking to learn new things. In free moments I like to play video games, watch movies and series. During graduation I participated in Scientific Initiation projects. The first one, Improvement of collimation systems for gamma-particle coincidence measurements, consisted of calculating Rutherford collisions by means of programming in C language and whose objective was to understand the operation of the collimator of the accelerator Pelletron from University of São Paulo. In the second one, the focus was on the High Energy Physics area at LHC at CERN with case studies. I worked for a while in the financial market, interning at the trading desk. After that, I dedicated myself for a few years to teaching Physics. On this page you will find some of the projects that I am currently working on. You can see my CV Here. "],["gpd-and-energy-supply.html", "Project 1 GPD And Energy Supply 1.1 Introduction 1.2 Data 1.3 Data Analysis 1.4 Conclusion", " Project 1 GPD And Energy Supply Code: GitHub 1.1 Introduction GDP is an acronym that means Gross Domestic Product and it is something extremely important for nations, because it consists of the sum of a countrys monetary values. In this way, this means that all goods and other types of services that are produced in a given region are added together over a period of time to know whether they are growing or not. Thus, GDP is one of the most used indicators in the market economy, always aiming to carry out research on how all the economic activity of a country is doing. Equally important, measuring sustainability indicators allows plants to assess the positive impacts of power generation, as well as the economic, social and environmental gains provided by the project. Experts say combating climate change should be one of the pillars for post-pandemic economic recovery. This is due to the fact that sustainable investments can generate higher returns for the economy. In this sense, stimulating the generation of renewable energy in the plants is essential, both to reduce impacts on the environment and also for the plant to remain competitive even in challenging scenarios. And to assess this, it becomes even more important to monitor sustainability indicators on a daily basis. Thinking about the great importance that energy indicators and their research impact on GDP and the great world movements that are happening in the search for clean and renewable energy, it seems interesting to make a case study on this content. In this case study, some indicators of the last 10 years of the 15 largest economies in the world are analyzed. To do this, 3 datasets are used. A greater focus will be given to the analysis of Brazil in relation to other countries. The analysis is done using python with Jupyter Notebook. 1.2 Data 1.2.1 Energy Indicators Energy_Indicators.xls is a list of United Nations energy supply and renewable electricity production indicators for the year 2015 with the last update in 2017. Energy Indicators TABLE 1.1: Description of variables in the Energy Indicators dataset. Variable Description Country List of country Energy Supply (Petajoules) Energy supply in each country Energy Supply per Capita (Gigajoules) Energy supply per capita in each country % Renewable (%) Rate of renewable energy in the country 1.2.2 Country GDP world_bank.csv is a GDP data for countries between 1960 and 2019 (last updated) provided by the World Bank TABLE 1.2: Description of variables in the GDP dataset. Variable Description Country List of country Year (1960 until 2019) GDP 1.2.3 Energy Technology scimagojr-3.xlsx is a Engineering and Energy Technology data made available by the Scimago Journal &amp; Country Rank that are made available to the public and include journals and scientific indicators from developed countries based on the information contained in the Scopus database and that measure the impact and the influence of scientific publications, with the latest update being made available in 2019. Engineering and Energy Technology TABLE 1.3: Description of variables in the Country Rank data for Energy Engineering and Power Technology dataset. Variable Description Rank Rank of the country who have more citations Country Countries Country Documents Document numbers Citable documents Number of citable documents published by a journal in the three previous years. Citations Number of citations received in the selected year by a journal to the documents published in the three previous years. Self-citations Number of journals self-citations in the selected year to its own documents published in the three previous years. Citations per document Average citations per document in a 2 year period. H index journals number of articles (h) that have received at least h citations. Knowing which dataset we are going to work with, we can now look at them to get a better idea of how to proceed with the data. We start by looking at the first 5 lines of Energy Indicators TABLE 1.4: first 5 line of Energy Indicators. Country Energy Supply Energy Supply per Capita % Renewable Afghanistan 3.21e+08 10 78.6693 Albania 1.02e+08 35 100 Algeria 1.959e+09 51 0.55101 American Samoa nan  0.641026 Andorra 9e+06 121 88.6957 We can see that there is data that needs to be cleaned up. In these first lines, it is noted that Afghanistan and Andorra use a very high percentage of renewable energy. Next, well look at the GDP indicators TABLE 1.5: first 5 line of GDP Indicators. Country 2010 2011 2012  2018 2019 Aruba 2.3905e+09 2.54972e+09 2.53464e+09  nan nan Afghanistan 1.58566e+10 1.78043e+10 2.00016e+10  1.83539e+10 1.92911e+10 Angola 8.37995e+10 1.1179e+11 1.28053e+11  1.01353e+11 8.88157e+10 Albania 1.19269e+10 1.28908e+10 1.23198e+10  1.5147e+10 1.52792e+10 Andorra 3.44997e+09 3.6292e+09 3.18881e+09  3.21832e+09 3.15406e+09 Again we see some countries with values that are missing. I believe that it will not be a problem when the data was merged. Now lets analyze the data from the Scimago Journal and Country Rank data for Energy Engineering and Power Technology TABLE 1.6: first 5 line of the Country Rank data for Energy Engineering and Power Technology. Rank Country Region Documents Citable documents Citations Self-citations Citations per document H index 1 China Asiatic Region 235126 233883 1909601 1306438 8.12 224 2 United States Northern America 157811 154288 1940563 639345 12.3 333 3 Japan Asiatic Region 46032 45559 436961 109968 9.49 181 4 India Asiatic Region 39893 38848 368175 123446 9.23 171 5 United Kingdom Western Europe 38873 37780 536378 100038 13.8 208 After clearing all the data and renaming the country names so as not to duplicate the lines, we can immerse the data according to the first 15 placed in the ranking provided by Scimago Journal. We can see this table below. TABLE 1.7: All top Rank 15 Scimago Journal Countries. Country Rank Documents Citations Energy Supply % Renewable  2019 China 1 235126 1909601 1.27191e+11 19.7549  1.43429e+13 United States 2 157811 1940563 9.0838e+10 11.571  2.14332e+13 Japan 3 46032 436961 1.8984e+10 10.2328  5.08177e+12 India 4 39893 368175 3.3195e+10 14.9691  2.86893e+12 United Kingdom 5 38873 536378 7.92e+09 10.6005  2.82911e+12 Germany 6 32935 367356 1.3261e+10 17.9015  3.86112e+12 Russian Federation 7 31880 91906 3.0709e+10 17.2887  1.69988e+12 Canada 8 29633 491467 1.0431e+10 61.9454  1.73643e+12 Italy 9 23725 312631 6.53e+09 33.6672  2.00358e+12 South Korea 10 23451 279709 1.1007e+10 2.27935  1.64674e+12 France 11 22429 300015 1.0597e+10 17.0203  2.71552e+12 Iran 12 19371 242250 9.172e+09 5.70772  nan Spain 13 18882 312632 4.923e+09 37.9686  1.39349e+12 Australia 14 18077 263733 5.386e+09 11.8108  1.39657e+12 Brazil 15 18024 152380 1.2149e+10 69.648  1.83976e+12 After immersing the data, a 15-row, 20-column Data Frame was created. We can see the columns partially due to the limitation of the page, it can be seen in full how the immersion process was done and with all the data on the GitHub page 1.3 Data Analysis Now that we have cleaned up and merged the data, we can make inferences. 1.3.1 What are the top 15 countries for average GDP over the past 10 years? FIGURE 1.1: The distribution of the top 15 GDP country. As can be seen in the graph, in the average GDP of the last 10 years there is a sovereignty of the USA still in relation to the other countries with China and Japan just behind the top 3. Brazil is the 8th best GDP average among the 15 largest. 1.3.2 How much did the average GDP vary over the 10-year period for Brazil? GDP has changed by -369.0 billion dollars over the past 10 years FIGURE 1.2: Distribution of the variation in Brazil GPD over the last 10 years. As can be seen in the information above, Brazil over the past 10 years has had a negative change in GDP. The graph above shows that the biggest drop occurred between 2015 and 2016, which can be explained by the national political-economic crisis, where sets of economic measures, in addition to certain external factors and certain internal political events, which occurred during the Dilma Rousseff Government, which, added , resulted in a drop in activity. In 2020 another recession is expected due to the global financial crisis caused by the Covid-19 pandemic. 1.3.3 What is the average energy supply per capita? The average energy supply is 157.6 Petajoules 1.3.4 What percentage do countries use of Renewable Energy? FIGURE 1.3: The distribution of the percentage fo countries use Renewable Energy. Analyzing the chart above, we see that Brazil is the country that has the highest percentage of use of renewable energy. According to the International Energy Agency, Brazil is the third largest renewable energy generator in the world and according to the Energy Research Company (EPE), the country has a predominant renewable energy electrical matrix, with emphasis on the hydraulic generation which accounts for 68.1% of the domestic supplyand is one of the great Brazilian forces when it comes to renewable energy, thus justifying this high percentage. 1.3.5 What is the estimated population size using Energy Supply and Energy Supply per capita? FIGURE 1.4: The distribution of the Estimated Population in Billions of people. As the data were not available due to the size of the population, a simple way is to use the Energy Supply and Energy Supply values per capita to make an estimate. Doing a quick search on the internet is easy to see that the figures are very close to the real estimate of the population of these countries. We see that China has almost 1.4 billion people, followed by India with 1.3 billion and then the USA with approximately 300 million. 1.3.6 What is the proportion of self-citations to the total citations? FIGURE 1.5: The distribution of the rank of the country with more self citations. Analyzing now more data related to Scimago Journal Rank, which is a measure of the scientific influence of academic journals that accounts for the number of citations received for a period and the importance or prestige of the journals from which these situations come, we see that China is the country which has a higher proportion of self-quotes on Energy and Telecommunications articles showing how interested China has been in this area in recent years. In Latin America, this impact is very visible with the recent revolution in the electricity transmission and distribution sector with the arrival of the Chinese state conglomerates State Grid and Three Gorges in the markets of Chile and Peru and which continue to expand their business in Brazil. 1.3.7 What is the correlation between the number of documents cited per capita and the supply of energy per capita? An interesting analysis that can be seen with the data provided is whether there is a correlation between the number of documents cited per capita and the energy supply and we can see that there is a great relationship between these aspects what it is 0.75. We see then that the greater the interest of a country in studies related to Energy, the greater is also the energy supply of that same country. 1.4 Conclusion After analyzing the dataset separately, we were able to use one of the great advantages of python to analyze data is the joining of data. With Datasets together we were able to make interesting inferences about the data. From GPDs at home in two countries to the correlation that there is energy supply and the number of citations visualizing graphs and data, showing the great power that the Python language has in data analysis and proving to be a great tool for Data Science. "],["medical-insurance-costs.html", "Project 2 Medical Insurance Costs 2.1 Introduction 2.2 Data 2.3 Exploring the Data 2.4 Linear Regression 2.5 Visualization of the fitted models 2.6 Conclusion", " Project 2 Medical Insurance Costs Code: GitHub 2.1 Introduction There is no doubt that health-related benefits are among the most important that a company can offer its employees. After all, they help to significantly reduce medical expenses and guarantee the necessary support in times of difficulty. All of this reflects on the collective well-being and brings positive results for the company. Among the great advantages of hiring health insurance in organizations, we can mention the greater engagement of the team. This is because employees who have access to quality medical care work with more attention, tranquility and disposition, achieving their best performance during working hours. Because Medical insurance is a much desired benefit, its offer also helps in attracting and retaining the best professionals in the market. And with great talents working in the company, it tends to become a benchmark for quality in what it does. In this project we analyses data from US Medical Insurance Costs. Recently many people are looking for this type of insurance and prices can vary widely according to the users data. The main question of this analysis is to answer the final price that users need to pay. The dataset insurance.csv was provided by Kaggle.com: Medical Cost Personal Datasets 2.1.1 Project Goals In this project are make a case study using the linear regression method on Medical Insurance Costs dataset provided by the kaggle 2.1.2 Analysis This solution uses descriptive and inferential statistics and data visualization to find key figures in understanding the distribution, count, and relationship between variables. 2.1.3 Evaluation The project concludes with the evaluation of a linear model. A simple case study is done for one variable and then for multiple variables. After that is focused on visualizing the adjusted models then look at the residual plots and the error band. 2.2 Data The project has one dataset provided by Kaggle called insurance.csv. In the data, each row represents an user and the columns are the responses to their user profiles which include data about them. insurance has 1,338 rows and 7 columns, this is a good amount of data to analyze. Each line, or sample, consists of the following features: TABLE 2.1: Description of variables in the dataset. Variable Description age Age of primary beneficiary sex Clients gender bmi Body mass index of the client. BMI is a measure of a persons weight with respect to their height. children Numbers of children depending on the client smoker smoker yes or no region Region of which the customer is a part charges Annual charge of how much the customer must pay 2.3 Exploring the Data Now lets take a look at the first 5 lines of the Data Frame that we want to analyze. TABLE 2.2: The 5 first lines of the Data Frame Insurance. age sex bmi children smoker region charges 19 female 27.9 0 yes southwest 16884.9 18 male 33.77 1 no southeast 1725.55 28 male 33 3 no southeast 4449.46 33 male 22.705 0 no northwest 21984.5 32 male 28.88 0 no northwest 3866.86 Before doing a deeper analysis on the data we can see some of the data and how it is distributed from graphs. We started by looking at the Age Histogram. FIGURE 2.1: The distribution of the Age Histogram. Then, we can take a look at the scatter graph in the age variable x charge and separated between smokers and non-smokers, as we believe it has a great impact on the value of the final price charged. FIGURE 2.2: Scatter plot of chargers vs age between smokers and non-smokers. No-Smoker = 0 and Smoker = 1 As can be seen in FIGURE 2.2, we have a leap in value when we analyze the graphs between smokers and non-smokers. Furthermore, we can see that there is a linear growth between age and the amount charged. Next, lets look at the BMI distribution: FIGURE 2.3: The BMI distribution. The BMI values are all well distributed in Gaussian. We see that there are no outliers to be lipated. Knowing that BMI has a well-behaved distribution, we will take a look at how BMI is influenced between the processed value between treatment and untreated. FIGURE 2.4: Scatter plot of charges vs BMI between smokers and non-smokers. Analyzing FIGURE 2.1, we can see that there is a linear growth between BMI and the amount charged when they are smokers, which is not the case for non-smokers. Taking a first look at the data provided and some plotted graphs, we can see that there are probably relationships between some variables provided and the charge amount. Next we will investigate it. 2.4 Linear Regression We initially focus on regression models where the amount charged is the dependent variable. That is, we predict the amount charged based on other variables. Since the feature charge is a quantitative variable, we model it using linear regression. 2.4.1 Interpreting regression parameters in a basic model We start with a simple linear regression model with only one covariable, age, to predict the charge. The model that is adjusted expresses the amount charged as a linear function of age. TABLE 2.3: Linear Regression Model adjusted for age variable using the python statsmodels library. The part that starts to be relevant to us of the Table presented is from coef. This section contains the estimated values for the parameters of the regression model. Its pattern, errors and other values are used to quantify the uncertainty in the estimates of the regression parameters. This adjusted model implies that when comparing two people of different ages in one year, the oldest person will have an average of 257. 72 units of charges larger than the youngest person. This difference is statistically significant, based on the p-value showing in the column labeled P&gt; | t |. This means that there is strong evidence that there is a real association between age and charges made by the customer. To better understand the meaning of the regression parameter 257.7226, we can look at the standard deviation of charges. The standard deviation of around $ 12,110 is large compared to the slope of the 258 regression. However, the slope of the regression corresponds to the average change in collection for a single year of age, and this effect accumulates with age. Comparing a 40 year old person with a 60 year old person, there is a 20 year difference, which translates into 20 * 258 = 5,160 unit difference in the average value charged between these two people. This difference is about less than half a standard deviation, and is generally considered to be an important and significant change. 2.4.2 R-square In the case of regression with a single independent variable, as we have here, there is a very close correspondence between the regression analysis and Pearsons correlation analysis. The R-square is a statistical measure of how close the data is to the adjusted regression line. The value obtained was 0.09, which means that 9% of the variation in the amount charged is explained by the age of the customers. 2.4.3 Adding more variables to the model After analyzing a simple linear regression model with only one covariant (age), we use all the variables. The real power of regression happens when we have more than one covariant to predict an output. TABLE 2.4: Linear Regression Model adjusted for all variables using the python statsmodels library. The model that was adjusted in TABLE 2.4 uses all parameters to explain the variation in the amount charged. We can see how interesting the value of R-square is. If we look at the table, we see a value of 0.75, that is, we have a linear regression model that 75% of the variation in the amount charged is probably explained by the variables provided. Looking at the age coefficient, which we studied earlier, there were no changes even adding many other variables. Thus, we can notice that the age variable has no relation with the other variables provided in the model. It is possible to confirm this result by looking at the value of the correlations between the variables and noting that there is almost no relationship between age and the rest of the other variables. FIGURE 2.5: Correlation between variables 2.5 Visualization of the fitted models In this section we demonstrate some graphing techniques that can be used to gain a better understanding of a regression model that has been fit to data. We start with plots that allow us to visualize the fitted regression function, that is, the mean charges expressed as a function of the covariates. These plots help to show the estimated role of one variable when the other variables are held fixed. We will also plot 95% simultaneous confidence bands around these fitted lines. Although the estimated mean curve is never exact based on a finite sample of data, we can be 95% confident that the true mean curve falls somewhere within the shaded regions of the plots below. This type of plot requires us to fix the values of all variables other than the independent variable (Charges here), and one independent variable that we call the focus variable (which is age here). Below we fix the gender as female and the BMI as 25. Thus, the graphs below show the relationship between expected Charges and age for women with BMI equal to 25. The plot analyzed for the number of children is shown below. Age is 50 with the other parameters the same. FIGURE 2.6: Fitted regression function charge vs age. The analogous plot for Number of Children is shown next. Here we fix the gender as female and the age at 50, so we are looking at the relationship between expected charge and age for women of age 50. FIGURE 2.7: Fitted regression function charge vs number of children. The error band for the number of children is noticeably greater than the error band for the age, indicating that there is less certainty in relation to children and charge compared to age and charge. The discussion so far has primarily focused on the mean structure of the population, that is, the model for the average change of a person with a given age, gender, BMI, smoker, region and children. A regression model can also be used to assess the variance structure of the population, that is, how much and in what manner the observations deviate from their mean. We will focus on informal, graphical methods for assessing this. To begin with, we plot the residuals against the fitted values. Recall that the fitted values are the estimated means for each observation, and the residuals are the difference between an observation and its fitted mean. For example, the model may estimate that a 50 year old female will have on average an charge of 32, 000 $. But a specific 50 year old female may have a blood pressure of 110 or 150, for example. The fitted values for both of these women are 32, 000, and their residuals are -10,000, and 10,000, respectively. The simplest variance pattern that we can see in a linear regression occurs when the points are scattered around the mean, with the same degree of scatter throughout the range of the covariates. When there are multiple covariates, it is hard to assess whether the variance is uniform throughout this range, but we can easily check for a mean/variance relationship, in which there is a systematic relationship between the variance and the mean, i.e. the variance either increases or decreases systematically with the mean. The plot of residuals on fitted values is used to assess whether such a mean/variance relationship is present. FIGURE 2.8: Fitted values vs Residuals. Above we show the plot of residuals on fitted values for the Insurance data. It appears that we have a modestly increasing mean/variance relationship. 2.6 Conclusion As can be seen, the linear regression model is a very powerful tool for analyzing model predictions if more than one variable is used. Using the python statsmodels library, we saw how easy and very useful it is to analyze statistical models. "],["quantium-retail-strategy-and-analytics.html", "Project 3 Quantium Retail Strategy and Analytics 3.1 Introduction 3.2 Data 3.3 Exploring the Data 3.4 Data analysis on customer segments 3.5 Deep dive into specific customer segments for insights 3.6 Conclusion", " Project 3 Quantium Retail Strategy and Analytics Code: GitHub 3.1 Introduction This work is part of the virtual internship program of the company Quantium. Quantium is a Data Science company that helps companies with insights and models looking to improve their performance. Quantium has had a data partnership with a major supermarket brand in recent years, which provides transactional and customer data. The analysis is based on Quantium chip data to better understand the types of customers who buy chips and their buying behavior in the region. The analysis insights serve to feed the supermarkets strategic plan for the chip category. 3.2 Data The first step in any analysis is to first understand the data. Lets take a look at each of the datasets provided. 3.3 Exploring the Data Starting with the analysis of the dataset transaction, lets look at the first few lines. 3.3.1 Examining transaction data DATE STORE LYLTY_CARD TXN_ID PROD NAME QTY TOT_SALES 43390 1 1000 1 5 Natural Chip Compny SeaSalt175g 2 6 43599 1 1307 348 66 CCs Nacho Cheese 175g 3 6.3 43605 1 1343 383 61 Smiths Crinkle Cut Chips Chicken 170g 2 2.9 43329 2 2373 974 69 Smiths Chip Thinly S/Cream&amp;Onion 175g 5 15 43330 2 2426 1038 108 Kettle Tortilla ChpsHny&amp;Jlpno Chili 150g 3 13.8 As we are only interested in words that tell us if the product is chips or not, lets remove all words with digits and special characters such as &amp; from our set of product words. There are salsa products in the dataset but we are only interested in the chips category, so lets remove these. Next, we can check summary statistics such as mean, min and max values for each feature to see if there are any obvious outliers in the data and if there are any nulls in any of the columns STORE QTY TOT_SALES PROD_SIZE count 246740 246740 246740 246740 mean 135.05 1.90646 7.31611 175.584 std 76.787 0.342499 2.4749 59.4321 min 1 1 1.7 70 25% 70 2 5.8 150 50% 130 2 7.4 170 75% 203 2 8.8 175 max 272 5 29.5 380 There are no nulls in the columns but product quantity appears to have an outlier which we should investigate further. Lets investigate further the case where 200 packets of chips are bought in one transactions. DATE STORE LYLTY_CARD TXN_ID PROD PROD_NAME QTY TOT_SALES PROD_SIZE 2018-08-19 226 226000 226201 4 dorito corn chp supreme 200 650 380 2019-05-20 226 226000 226210 4 dorito corn chp supreme 200 650 380 There are two transactions where 200 packets of chips are bought in one transaction and both of these transactions were by the same customer. Lets see if the customer has had other transactions DATE STORE LYLTY_CARD TXN_ID PROD NAME PROD_QTY TOT_SALES PROD_SIZE 2018-08-19 226 226000 226201 4 dorito corn chp supreme 200 650 380 2019-05-20 226 226000 226210 4 dorito corn chp supreme 200 650 380 It looks like this customer has only had the two transactions over the year and is not an ordinary retail customer. The customer might be buying chips for commercial purposes instead. We remove this loyalty card number from further analysis. STORE QTY TOT_SALES PROD_SIZE count 246740 246740 246740 246740 mean 135.05 1.90646 7.31611 175.584 std 76.787 0.342499 2.4749 59.4321 min 1 1 1.7 70 25% 70 2 5.8 150 50% 130 2 7.4 170 75% 203 2 8.8 175 max 272 5 29.5 380 Thats better. Now,looking at the number of transaction lines over time to see if there are any obvious data issues such as missing data, we see that is missing a value. Theres only 364 rows, meaning only 364 dates which indicates a missing date. figure 1 Zooming in on the data to take a closer look: We can see that the increase in sales occurs in the lead-up to Christmas and that there are zero sales on Christmas day itself. This is due to shops being closed on Christmas day. Now that we are satisfied that the data no longer has outliers, we can move on to creating other features such as brand of chips or pack size from PROD_NAME. We start with pack size. The largest size is 380g and the smallest size is 70g - seems sensible! Lets plot a histogram of PACK_SIZE since we know that it is a categorical variable and not a continuous variable even though it is numeric. figure3 3.3.2 Examining customer data Now that we are happy with the transaction dataset, lets have a look at the customer dataset. LYLTY_CARD_NBR LIFESTAGE PREMIUM_CUSTOMER 1000 YOUNG SINGLES/COUPLES Premium 1002 YOUNG SINGLES/COUPLES Mainstream 1003 YOUNG FAMILIES Budget 1004 OLDER SINGLES/COUPLES Mainstream 1005 MIDAGE SINGLES/COUPLES Mainstream We will now join the two data using the python merge function. DATE LYLTY_CARD_NBR TXN_ID PROD_NBR PROD_NAME QTY TOT_SALES SIZE LIFESTAGE PREMIUM_CUSTOMER 2018-10-17 1000 1 5 natural chip compny seasalt 2 6 175 YOUNG SINGLES/COUPLES Premium 2019-05-14 1307 348 66 ccs nacho cheese 3 6.3 175 MIDAGE SINGLES/COUPLES Budget 2019-05-20 1343 383 61 smiths crinkle cut chips chicken 2 2.9 170 MIDAGE SINGLES/COUPLES Budget 2018-08-17 2373 974 69 smiths chip thinly cream onion 5 15 175 MIDAGE SINGLES/COUPLES Budget 2018-08-18 2426 1038 108 kettle tortilla chpshny jlpno chili 3 13.8 150 MIDAGE SINGLES/COUPLES Budget As the number of rows in result is the same as that of transactionData, we can be sure that no duplicates were created. This is because we created result by setting a left join which means take all the rows in transactionData and find rows with matching values in shared columns and then join the details in these rows to the x or the first mentioned table. 3.4 Data analysis on customer segments Now that the data is ready for analysis, we can define some metrics of interest to the client: - Who spends the most on chips (total sales), describing customers by lifestage and how premium their general purchasing behaviour is - How many customers are in each segment - How many chips are bought per customer by segment - Whats the average chip price by customer segment Lets start with calculating total sales by LIFESTAGE and PREMIUM_CUSTOMER and plotting the split by these segments to describe which customer segment contributes most to chip sales. TOT_SALES PREMIUM_CUSTOMER LIFESTAGE Budget MIDAGE SINGLES/COUPLES 33345.70 NEW FAMILIES 20607.45 OLDER FAMILIES 156863.75 OLDER SINGLES/COUPLES 127833.60 RETIREES 105916.30 YOUNG FAMILIES 129717.95 YOUNG SINGLES/COUPLES 57122.10 Mainstream MIDAGE SINGLES/COUPLES 84734.25 NEW FAMILIES 15979.70 OLDER FAMILIES 96413.55 OLDER SINGLES/COUPLES 124648.50 RETIREES 145168.95 YOUNG FAMILIES 86338.25 YOUNG SINGLES/COUPLES 147582.20 Premium MIDAGE SINGLES/COUPLES 54443.85 NEW FAMILIES 10760.80 OLDER FAMILIES 75242.60 OLDER SINGLES/COUPLES 123537.55 RETIREES 91296.65 YOUNG FAMILIES 78571.70 YOUNG SINGLES/COUPLES 39052.30 Plotting the bar graph of the data, we have: Sales are coming mainly from Budget - older families, Mainstream - young singles/couples, and Mainstream - retirees Lets see if the higher sales are due to there being more customers who buy chips. TOT_SALES PREMIUM_CUSTOMER LIFESTAGE Budget MIDAGE SINGLES/COUPLES 35514.80 NEW FAMILIES 21928.45 OLDER FAMILIES 168363.25 OLDER SINGLES/COUPLES 136769.80 RETIREES 113147.80 YOUNG FAMILIES 139345.85 YOUNG SINGLES/COUPLES 61141.60 Mainstream MIDAGE SINGLES/COUPLES 90803.85 NEW FAMILIES 17013.90 OLDER FAMILIES 103445.55 OLDER SINGLES/COUPLES 133393.80 RETIREES 155677.05 YOUNG FAMILIES 92788.75 YOUNG SINGLES/COUPLES 157621.60 Premium MIDAGE SINGLES/COUPLES 58432.65 NEW FAMILIES 11491.10 OLDER FAMILIES 81958.40 OLDER SINGLES/COUPLES 132263.15 RETIREES 97646.05 YOUNG FAMILIES 84025.50 YOUNG SINGLES/COUPLES 41642.10 Plotting again: There are more Mainstream - young singles/couples and Mainstream - retirees who buy chips. This contributes to there being more sales to these customer segments but this is not a major driver for the Budget - Older families segment. Higher sales may also be driven by more units of chips being bought per customer. Lets have a look at this next. Mainstream mid aged and young singles and couples are more willing to pay more per packet of chips compared to their budget and premium counterparts. This may be due to premium shoppers being more likely to buy healthy snacks and when they buy chips, this is mainly for entertainment purposes rather than their own consumption. This is also supported by there being fewer premium mid aged and young singles and couples buying chips compared to their mainstream counterparts. As the difference in average price per unit isnt large, we can check if this difference is statistically different. The t-test results &lt; 2.2e-16, i.e. the unit price for mainstream, young and mid-age singles and couples are significantly higher than that of budget or premium, young and mid age singles and couples. 3.5 Deep dive into specific customer segments for insights We have found quite a few interesting insights that we can dive deeper into. We might want to target customer segments that contribute the most to sales to retain them or further increase sales. Lets look at Mainstream - young singles/couples. For instance, lets find out if they tend to buy a particular brand of chips. MIDAGE SINGLES/COUPLES smiths crinkle chips salt vinegar 194 cheezels cheese 186 doritos corn chips nacho cheese 179 kettle chilli 179 cobs popd sour crm chives chips 176 YOUNG SINGLES/COUPLES tostitos splash of lime 335 kettle mozzarella basil pesto 332 doritos corn chips cheese supreme 326 smiths crnkle chip orgnl big bag 323 kettle tortilla chpshny jlpno chili 323 We can see that : - Mainstream young singles/couples are 23% more likely to purchase Tyrrells chips compared to the rest of the population - Mainstream young singles/couples are 56% less likely to purchase Burger Rings compared to the rest of the population Lets also find out if our target segment tends to buy large packs of chips. MIDAGE SINGLES/COUPLES 175.0 2975 150.0 1777 134.0 1159 110.0 1124 170.0 882 MIDAGE SINGLES/COUPLES 175.0 2975 150.0 1777 134.0 1159 110.0 1124 170.0 882 Both the segment buy 175g, 150g and 134g packets mostly 3.6 Conclusion Sales have mainly been due to Budget - older families, Mainstream - young singles/couples, and Mainstream - retirees shoppers. We found that the high spend in chips for mainstream young singles/couples and retirees is due to there being more of them than other buyers. Mainstream, midage and young singles and couples are also more likely to pay more per packet of chips. This is indicative of impulse buying behavior. Weve also found that Mainstream young singles and couples are 23% more likely to purchase Tyrrells chips compared to the rest of the population. The Category Manager may want to increase the categorys performance by off-locating some Tyrrells and smaller packs of chips in discretionary space near segments where young singles and couples frequent more often to increase visibility and impulse behavior. Quantium can help the Category Manager with recommendations of where these segments are and further help them with measuring the impact of the changed placement. Well work on measuring the impact of trials in the next task and putting all these together in the third task. "],["ok-cupid.html", "Project 4 Ok Cupid 4.1 Introduction 4.2 Data 4.3 Exploring the Data 4.4 Data Preparation 4.5 Prediction 4.6 Conclusion", " Project 4 Ok Cupid Code: GitHub 4.1 Introduction This project analyzes data from on-line dating application OKCupid. In recent years, there has been a massive rise in the usage of dating apps to find love. Many of these apps use sophisticated data science techniques to recommend possible matches to users and to optimize the user experience. These apps give us access to a wealth of information that weve never had before about how different people experience romance. The dataset okcupid_profiles.csv was provided by Kaggle.com: OkCupid Profiles 4.1.1 Project Goals In this project, the goal is to analyze the data from Kaggle using tools of Data Science. The primary research question that will be answered is whether an OkCupids user astrological sign can be predicted using other variables from their profiles. 4.1.2 Analysis This solution uses descriptive statistics and data visualization to find key figures in understanding the distribution, count, and relationship between variables. Since the goal of the project is to make predictions on the users astrological signs, classification algorithms from the supervised learning family of machine learning models are implemented. 4.1.3 Evaluation The project concludes with the evaluation of the machine learning model selected with a validation data set. The output of the predictions can be checked through a confusion matrix, and metrics such as accuracy, precision, recall, F1 and Kappa scores. 4.2 Data The project has one data set provided by Kaggle called okcupid_profiles.csv. In the data, each row represents an OkCupid user and the columns are the responses to their user profiles which include multi-choice and short answer questions. To analyze the user profiles from OkCupid, pandas will be used to load the dataset into a DataFrame so that it can be explored and visualized with Python. 4.2.1 Data Characteristics profiles has 59,946 rows and 31 columns, this is a good sign since there seems to be enough data for machine learning. The columns in the dataset include: TABLE 4.1: Description of variables in the dataset. Variable Description age categorical variable of educational attainment body_type categorical variable of body type of user diet categorical variable of dietary information drinks categorical variable of alcohol consumption drugs categorical variable of drug usage education categorical variable of educational attainment ethnicity categorical variable of ethnic backgrounds height continuous variable of height of user income continuous variable of income of user job categorical variable of employment description offspring categorical variable of children status orientation categorical variable of sexual orientation pets: categorical variable of pet preferences religion categorical variable of religious background sex categorical variable of gender sign categorical variable of astrological symbol smokes categorical variable of smoking consumption speaks categorical variable of language spoken status categorical variable of relationship status last_online date variable of last login location categorical variable of user locations And a set of open short-answer responses to : TABLE 4.2: Description of variables short-answer in the dataset. Variable Description essay0 My self summary essay1 What Im doing with my life essay2 Im really good at essay3 The first thing people usually notice about me essay4 Favorite books, movies, show, music, and food essay5 The six things I could never do without essay6 I spend a lot of time thinking about essay7 On a typical Friday night I am essay8 The most private thing I am willing to admit essay9 You should message me if 4.3 Exploring the Data Lets start by looking at the first rows and columns of our dataset: TABLE 4.3: The 5 first lines of the Data Frame. age status sex orientation body_type drinks drugs education 22 single m straight a little extra socially never working on college/university 35 single m straight average often sometimes working on space camp 38 available m straight thin socially nan graduated from masters program 23 single m straight thin socially nan working on college/university 29 single m straight athletic socially never graduated from college/university First to be explored is the number of unique signs, and the values. It seems that there are 48, but there should only be 12 signs. It is important that we clean the labels since this is what will be predicted and 48 predictions would be quite difficult. By taking the first word of the column, the signs can be saved without the qualifiers. The qualifiers could be used for another problem down the line. After adjusting the signs, we can better analyze them on our dataset. 4.3.1 Continuous Variables 4.3.1.1 age The next plot shows the distribution of age in the group. It seems that most users are in their late 20s to early 30s. Here is the same chart but broken down by gender. It seems that there are proportionally similar breaks of gender by age, but slightly fewer females overall. 4.3.1.2 Height The next plot shows the height variable. most people look like they are between 1.5 and 2 meters tall. Here is the same height chart showing the breakdown by gender. It seems obvious, but females tend to be shorter than males and look to have a normal distribution. 4.3.1.3 Income Here is the data of income, it seems that the majority of the participants do not include their income figures. 4.3.2 Discrete Variables 4.3.2.1 Sex Previously it was identified that there are more males in the data, and it seems that there are ~35,000 men to ~25,000 women. 4.3.2.2 Body Type The next chart shows the body type variable, and it seems that most users will describe themselves as average, fit, or athletic. The next chart shows the breakdown of body type by gender and it seems that some of the body type descriptions are highly gendered. For example curvy and full figured are highly female descriptions, while males use a little extra, and overweight more often. 4.3.2.3 Diet Here is a chart of the dietary information for users. Most users eat mostly anything, followed by anything, and strictly anything, being open-minded seems to be a popular signal to potential partners. 4.3.2.4 Drinks The next plot shows that the majority of the users drink socially, then rarely and often. 4.3.2.5 Drugs The vast majority of users never use drugs. 4.3.2.6 Smoking Similarly for drugs the majority of users chose no for smoking. 4.3.2.7 Education Below you can see the majority of users are graduates from college/university followed by masters programs and those working on college/university. Interestingly space camp related options are fairly popular options. 4.3.2.8 Jobs Most users dont fit into the categories provided, but there are a fair share of students, artists, tech, and business folks. 4.3.2.9 Offspring The data suggest that most users do not have kids. 4.3.2.10 Orientation The majority of users are straight. 4.3.2.11 Pets The chart shows that most users like or have dogs. 4.3.2.12 Religion Religion was similar to sign where there are a lot of qualifiers. religion was cleaned to take the first word and distilled down to 9 groups. The majority was not very religious identifying as agnostic, other, or atheists. 4.3.2.13 Signs Here are the astrological signs of the users. They are mainly evenly distributed with Capricorns being the rarest and Leos being the most common. 4.3.2.14 Status The relationship status for a dating website is fairly predictable. One would assume that most people are single and available which is reflected in the data. 4.4 Data Preparation Missing data is often not handled by machine learning algorithms well and have to be checked so they may need to be imputed or removed. It seems that many of the columns do have missing values. Preparing the data for modeling is important since it can speed up the process and produce better models. As the adage goes, garbage in garbage out so we want to make sure the data we are inputting into our modelling step is good enough to share with others. The data for the model is going to be a subset of the variables. The variables were selected because they might be a good predictor for astrological signs, where some of the variables that were not selected such as age is probably not a good indicator. Missing values are dropped to create a fully complete data set. Furthermore, an imbalance in the prediction label needs to be checked. This is important since its a multi-class problem where two or more outcomes can be bad. An imbalance in a response variable is bad since it means that some labels only occur a few times. This is an issue for machine learning algorithms if there is not enough data to train with which will give bad predictions. In the given dataset, we observe that the counts of all the zodiac signs are more or less equal (i.e., without large deviations). Hence, we do not have to worry about imbalances and trying to address this problem. Next the data was split into train and validation sets. In this split 25% of the data is reserved for the final validation, while 75% is kept for training the model. 4.5 Prediction 4.5.1 Model building Now its time to create some models, here is a list of Multi class models available in scikit learn. For this project three common algorithms will be used to make predictions. Below, the respective modules for Logistic Regression, Decision Trees, and KNN are loaded. 4.5.2 Evaluation Metrics In the models, there will be several values that can be evaluated below is a quick diagram: here is a quick description of the metrics: Accuracy: is the correct values divided by total values Precision: is the True Positives divided by the sum of True Positives and False Negatives. So precision is the values of the true positives divided by the actual positive values. Recall: is the True Positives divided by the sum of True Positives and False Positives. So recall is the values of the true positives divided by the positive guesses. F1-score: is a blended score of precision and recall which balances both values. Macro Avg: is the unweighted mean value of precision and recall. Weighted Avg: is the weighted mean value of precision and recall by the support values for each class. Support: is the number of observations in class to predict. 4.5.3 Logistic Regression The first model is using logistic regression with the multi_class=\"multinomial\" argument. Using lr_model predictions are created from the training dataset which is used to figure out how well the model performed. precision recall f1-score support aquarius 0.13 0.06 0.08 930 aries 0.13 0.11 0.12 1012 cancer 0.13 0.20 0.15 1067 capricorn 0.14 0.05 0.08 921 gemini 0.12 0.12 0.12 1116 leo 0.12 0.18 0.15 1123 libra 0.13 0.12 0.13 992 pisces 0.12 0.11 0.12 1026 sagittarius 0.12 0.06 0.08 993 scorpio 0.12 0.08 0.09 1013 taurus 0.12 0.14 0.13 1051 virgo 0.13 0.21 0.16 1095 accuracy 0.12 12339 macro avg 0.12 0.12 0.12 12339 weighted avg 0.12 0.12 0.12 12339 The final accuracy of the logistic regression model is 12% which is terrible considering a random guess should result in being correct ~8% of the time (1/12). 4.5.4 K Nearest Neighbor The next model is the KNeighborsClassifier which will take 20 of its neighbors to predict the signs. The default value for n_neighbors is 5 which was kept. This number can be tuned later on if needed. precision recall f1-score support aquarius 0.26 0.66 0.37 930 aries 0.26 0.53 0.35 1012 cancer 0.30 0.44 0.35 1067 capricorn 0.34 0.37 0.35 921 gemini 0.34 0.32 0.33 1116 leo 0.40 0.28 0.33 1123 libra 0.38 0.23 0.29 992 pisces 0.42 0.25 0.31 1026 sagittarius 0.43 0.23 0.30 993 scorpio 0.42 0.22 0.29 1013 taurus 0.42 0.25 0.31 1051 virgo 0.42 0.23 0.29 1095 accuracy 0.33 12339 macro avg 0.36 0.33 0.32 12339 weighted avg 0.37 0.33 0.32 12339 This model had a 33% accuracy which is a good sign. 4.5.5 Decision Trees The last model is the decision tree, the default max_depth is none which means that it will If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.. precision recall f1-score support aquarius 0.66 0.94 0.78 930 aries 0.67 0.89 0.76 1012 cancer 0.70 0.87 0.78 1067 capricorn 0.73 0.84 0.78 921 gemini 0.76 0.79 0.78 1116 leo 0.80 0.79 0.80 1123 libra 0.79 0.76 0.78 992 pisces 0.81 0.74 0.77 1026 sagittarius 0.88 0.70 0.78 993 scorpio 0.89 0.71 0.79 1013 taurus 0.94 0.67 0.78 1051 virgo 0.91 0.67 0.77 1095 accuracy 0.78 12339 macro avg 0.80 0.78 0.78 12339 weighted avg 0.80 0.78 0.78 12339 The results are very promising because it has a 78% accuracy with this model. Below is a confusion matrix of the results with the true values on the y axis and predicted values along the x axis. Since the diagonals are lighter in color and have higher numbers, the accuracy is going to be high since those are the True Positives. Going back to the model, a quick analysis shows that this tree model has a depth of 65 branches, which probably not generalize to another dataset. In this case this model has been overfit for this data. To make a point, a five fold cross validation is created with the same data. The results are worse than the KNN and about the Logistic Regression algorithms. the baseline was ~9% The decision tree model will be made again, but with a max_depth of 20 to stop the algorithm from reaching the stopping point. precision recall f1-score support aquarius 0.40 0.33 0.36 930 aries 0.40 0.27 0.32 1012 cancer 0.40 0.28 0.33 1067 capricorn 0.27 0.30 0.28 921 gemini 0.37 0.26 0.31 1116 leo 0.37 0.25 0.30 1123 libra 0.59 0.19 0.29 992 pisces 0.19 0.44 0.26 1026 sagittarius 0.40 0.23 0.29 993 scorpio 0.64 0.20 0.30 1013 taurus 0.39 0.25 0.31 1051 virgo 0.16 0.47 0.24 1095 accuracy 0.29 12339 macro avg 0.38 0.29 0.30 12339 weighted avg 0.38 0.29 0.30 12339 The new accuracy rate of ~41% is worse than the first iteration, but slightly better than the KNN model. If we check again with cross validation, the new model is still averaging ~8% which is not very good. precision recall f1-score support aquarius 0.08 0.20 0.11 332 aries 0.09 0.18 0.12 316 cancer 0.09 0.13 0.11 390 capricorn 0.05 0.06 0.05 276 gemini 0.08 0.08 0.08 380 leo 0.10 0.07 0.08 393 libra 0.08 0.05 0.06 362 pisces 0.07 0.04 0.05 308 sagittarius 0.09 0.04 0.06 319 scorpio 0.07 0.03 0.05 343 taurus 0.08 0.05 0.06 339 virgo 0.14 0.08 0.10 356 accuracy 0.08 4114 macro avg 0.09 0.08 0.08 4114 weighted avg 0.09 0.08 0.08 411 4.5.6 Final Model So it seems that the knn_model might be the best model for OkCupid to use when users dont have their signs listed on their user profile. By using the hold out or validation set, we get ~8% accuracy which is not very good. precision recall f1-score support aquarius 0.08 0.22 0.12 294 aries 0.09 0.18 0.12 345 cancer 0.09 0.15 0.11 328 capricorn 0.06 0.06 0.06 315 gemini 0.08 0.06 0.07 366 leo 0.10 0.07 0.08 395 libra 0.10 0.06 0.08 326 pisces 0.07 0.05 0.06 337 sagittarius 0.08 0.03 0.05 347 scorpio 0.08 0.04 0.05 355 taurus 0.07 0.04 0.05 345 virgo 0.08 0.05 0.06 361 accuracy 0.08 4114 macro avg 0.08 0.08 0.08 4114 weighted avg 0.08 0.08 0.07 4114 In the confusion matrix, it becomes clear that Cancer, Gemini, Leo, and Virgo was predicted most often, but were not super accurate since the vertical color band represents even distributed guesses mostly wrong and some correct. 4.6 Conclusion In this project, machine learning was used to predict the astrological signs of OkCupid users. This is an interesting feature, as many people believe in astrology and combinations between compatible star signs. If users do not enter their signals, an algorithmic solution may have generated a signal to input the missing data when making matches. Unfortunately, the final algorithm selected was no better than basic guessing, showing that not everything is appropriate for using machine learning as many people say out there. "],["real-estate-são-paulo-rent-prediction.html", "Project 5 Real Estate São Paulo Rent Prediction 5.1 Introduction 5.2 Data 5.3 Model 5.4 Prediction 5.5 Use Case of the Model 5.6 Conclusion", " Project 5 Real Estate São Paulo Rent Prediction Code: GitHub Web App: sp-rent-predictions 5.1 Introduction Everyone needs a place to live. It can be a house, a flat or an apartment. Everyone, at some point in life, is faced with the choice of buying or renting a house. On top of that, the following questions can be asked: Is buying a property worthwhile or staying in rent is the best alternative? In a society that values and idealizes the dream of home ownership, those who question themselves about the subject end up being seen as crazy. After all, from an early age, we are led to believe that home ownership is a goal that everyone should have and strive to achieve. This, however, is not an absolute truth. Furthermore, having a home, for many, is synonymous with stability and security. But, is it really worth buying a house? Or can continuing to rent make more sense? Lets point out some of the reasons that it makes sense to rent and why this is such an attractive business: Renting is not synonymous with losing money Considering the fixed income, depending on how the interest rates are, for example, it is more worthwhile to continue renting and invest the money that was reserved for the home itself. Thus, it is possible to pay the lease with the proceeds and save the rest to, who knows, buy it later. After all, the vast majority of people do not buy their house in cash and resort to financing. And this type of business is not advantageous, as the person ends up assuming a debt for years, and the amount to be paid for the property can even triple in comparison with the original value of the property. In this way, it can be much more worthwhile to invest your money - making it work for you - and to continue renting, paying this expense (or part of it) with the income. Property is not an investment It is important to keep in mind that owning a home is only for the sake of the family and does not generate any profit. With this new acquisition, debts such as property tax and other taxes, maintenance expenses and a series of repairs that may appear will come. Certainly your money will be invested there, but it can both appreciate - with the appreciation of the good itself over the years - as well as devalue itself. Not to mention that the money is immobilized, causing you to lose liquidity. The property is not yours until it is paid off The property is not yours! Yes, the apartment will be yours only after paying off the debt. That is, you will only be the owner after the twenty years that you were paying for the good! Is it worth it to spend all that time paying off a debt? Depending on the contract signed, you will only be the owner after a few decades In addition to the reasons mentioned above, São Paulo is a very busy city and many people go to it to work or study for a while, and renting makes a lot more sense than buying. Regardless of the reasons for renting, the sides need to agree on a price. It is always good to know how much a property is worth, what the expected transaction price is. 5.1.1 Analysis In this work, we want to find an answer to these questions. This case study is intended to be a comprehensive use case of how to deal with a regression problem for Data Science. We will start with some questions that allow us to understand the problems related to the price of renting a property. The owner does not know how to increase the value of the property so that the investment is less than the added value. For example, building a swimming pool can increase the price and forming a bathroom is not worth it. The owner does not know how much he is going to rent a property. He makes an offer on the portal and doesnt know if the price is right, The tenant does not know how much the property is worth. These are some of the questions that can be asked. As a definition of our problem, we define the propertys valuation, and through explanations we try to get an answer depending on the position we choose. Initially we will see how the literature says what the amount should be charged in a rent. According to, the basic rule most used to calculate rent is to apply a value between 0.5% and 1%, per month, on the market value of the property. This means that if the property is worth R$ 100 thousand, the rent must be between R$ 500 and R$ 1,000 per month. Research shows that, on average, two- and three-bedroom houses and apartments, the rental price is 0.75%. But there are many variables that make up the rent calculation. The real estate market itself regulates itself and pricing the rental price of properties by supply and demand. Supply and demand is an important concept of the Liberal Economy. The characteristics of the property itself are also decisive in the calculation of rent. Similar properties in the same region of the city, in general, have approximate values. That is why it is always important to observe the values in force in the real estate market in the region in which you will invest or prospect your property for business. There are cases of similar properties in the same neighborhood that may have different rentals. This is justified by the commercial and transport structure available. Factors such as the total length of the property, leisure facilities, swimming pool, party room, condominium, concierge services, security, garage, number of rooms influence the calculation of rent An apartment or house, for example, close to bus or metro stations may have a higher rental price than a more remote one. The same goes for properties in regions with supermarkets, pharmacies and bakeries located next to a park. With this premise, we can now analyze the data. 5.2 Data As a data source for information on property characteristics and prices, a set of data obtained from a web scraper on the Zap Imóveis website was used. Web Scraper is a tool with the purpose of extracting data from a HyperText Markup Language (HTML) website. The web scraper was developed by GeovRodri based on Beautiful Soup and made available through the link: zapimoveis-scraper The set of data collected on the website is based on rental advertisements in the city of São Paulo containing more than 60 thousand data. The data set obtained contains the data according to TABLE 5.1. TABLE 5.1.1: Description of variables in the dataset. Variable Description description property description price price (monthly) bedrooms number of bedrooms on property bathrooms number of bathrooms on property total_area_m2 property area (square meters) vacancies parking spots available on property address property address link link of the property Another idea on how to enrich our solution was to add external data. Our hypothesis took into account the information given in the introduction of how a property is calculated, therefore, a location of the property can significantly affect the price. Therefore, we also take into account the distance to the nearest public transport (Metro and bus), distance to the nearest school, number of cultural variety within 1km and addition to the crime rate in the region. 5.2.1 Data Preparation Like all automatic data collection processes, the web scraper described above generated data with quality problems. To solve this problem, we apply some cleaning processes. The first filter was to remove instances that had null values (absent) for attributes considered fundamental for the analysis. To eliminate outliers that left the data inconsistent, such as, for example, rent with sale price of properties, places with more than 50 bathrooms and houses with areas the size of a city, conditions were added for the property to be considered consistent with reality. 5.2.2 External data External data has been added. We believe that many other variables describe the rental price. Geospatial information was missing. Why do some places tend to be more expensive? Perhaps it is because of the availability of public transport. That is why we decided to add, for example, a variable that describes the distance to the nearest bus or metro station. In the data described, we note that unfortunately there is no zip code and geolocation of the properties. To our happiness, the website provides address, and from that it was possible to use a geolocation tool called Geopy available for Python. With it it is possible to enter an address and automatically the library provides you with the longitude and latitude of the location. After obtaining the geolocation values of the properties, a second cleaning was performed on the data. Many registrations and inconsistencies of Geopy cause some locations of the respective properties to be outside the city of São Paulo (curiously some even in the middle of the Atlantic Ocean). To solve this problem, we took the official data from the location of the city of São Paulo, made available by IBGE and with the help of Geopandas, we applied the data so that all the properties in our database were within the region. After another treatment of data, we added variables that describe the distances to the bus, subway station and school closest to the property. The data were obtained from the GeoSampa website provided by the city of São Paulo. Another reason we took into account for the price of renting the property was that they may be more expensive because of some interesting places around, such as museums, galleries, shopping centers or libraries. These places are not just stand-alone facilities, but are connected to other infrastructure, which is generally expected to increase the price. For this variable, we consider a radius of 700 meters chosen arbitrarily. The data were again obtained from the GeoSampa website. With the same situation in mind, we also added a variable called food to calculate the number of places to eat within a radius of 700 meters. We consider restaurants, bars, cafes, among others. To obtain the data, we use the Overpass Turbo site which is a web based data mining tool for OpenStreetMap. Finally, we also decided to add a crime rate variable around real estate, as we believe that more violent places tend to have lower rent values. The data were obtained through the Geospatial São Paulo Crime Database made available by Kaggle. The data can be viewed in FIGURE X. There are 21367 bus stops, 89 metro stations, 4761 schools, 1697 cultural points and 12899 crime data. TABLE 5.1.2: Description of the external variables in the dataset. Variable Description Id property Id Type house / apartment Region region of the city dist_subway distance from properly to the nearest subway station dist_bus distance from properly to the nearest bus station school distance from properly to the nearest school ncult number of cult place in area of the property food number of food place in area of the property crime% % of crime in property area latitude latitude of the property longitude longitude of the property items Items of the property FIGURE 5.1: Spatial external data. Stops Bus on the left and Subway Station on the right. Each point indicates the location of the property (marked in red). The blue and indicate the location of public transport stops. On the x-axis we have longitude values, while on the y-axis we have latitude values. FIGURE 5.2: Spatial external data. Schools on the left and cultural places on the right. FIGURE 5.3: Food Place on the left and heatmap of crime rate on the right Most cultural locations are located in the city center. Thus, this variable also tells the story of how much city center the property has. As all these places are in the center of the city, these other points must reflect some local centers. As can be seen in the figure, bus stops and schools are very dense across the city. Showing that all properties have a good connection to public transport and education. This can, for example, facilitate the movement of children to schools and parents to workplaces. But the problem of transport via the Metro is evident on the map. We see that it covers a very small region of the city, being that it is the fastest means of transport, which probably properties in its surroundings must have rents much more expensive than the rest of the region. In the last figure, we plot the density of crime in the city. We see that the center has a much higher density than the rest of the region. After doing this localization step, an id was created for the locations, noting that the property link exists a unique id for each one of them. This was important to eliminate duplicate data. The last step was to divide the data into training and test samples at random with a 70/30 ratio to apply machine learning. The script to download, clean and process the data can be found on GitHub. For the analysis of spatial data, a small geofast script was made. The distances between 2 arbitrary points on the earth can be obtained in the Geopy package for Python. However, the general equation is very computationally expensive for the purpose of this work. Then an adjustment was made in our case without losing almost any precision. A simple equation of geospatial calculation was used that can be applied for short distances of a few hundred kilometers. 5.2.3 Text Processing To extract information from the title and description, we followed the regular Natural Language Processing pipeline, which encompasses tokenization (splitting sentences into separate words), removal of common unhelpful words such as prepositions (stopwords removal), and lemmatization (reduction of words to their root forms). From the texts we add variables that we think are important for the price of the property. For example, if it is a townhouse, if it has a swimming pool, barbecue, if it is furnished and so on. 5.3 Model In this project, We will be using the linear model as a base model to finalize the data pre-processing steps. We will use outlier removal techniques, coding techniques, sizing / normalization techniques, resource engineering and different types of imputation combinations. We divided the modeling into five steps. Step1: Linear Models In this step We used linear models like Linear Regression, Bayesian Ridge Regression, Lasso, Elastic Net and Ridge. Step2: Support Vector Machines We used SVR(Support Vector Regressor). Similar to linear models for SVR also. Step3: Ensemble Methods In first two steps We used basic models, from step 3 onward models becomes complex. We used models like Gradient Boosting Regressor, LightGBM Regressor, XGB Regressor. Step4: Model Stacking Stacking is an ensemble learning technique to combine multiple regression models via a meta-regressor. At this stage we have good mix of multiple models, now is the best time to use StackingCVRegressor to combine all above models to improve the score even further. As expected this model resulted in best possible score! Step5: Model Blending Model blending is the manual step, where we manually adjust the weight for each model in order to ensemble the predictions to improve the score. Thumb rle is to give highest weights to best performing model and lowest to least performing model. After few trial and and error I could settle on the weights and manage to top my model stacking score! 5.3.1 Root Mean Square Error (RMSE) Root Mean square is the standard deviation of the residuals. Now lets understand what Standard deviation and residuals are. Standard deviation: Standard deviation is a measure of how spread out numbers are. Its formula is the square root of the Variance. Variance is defined as the average of the squared differences from the Mean. In the below formula of standard deviation xi= numbers, = Mean of the numbers and N = Total number of values Residuals: Residuals are a measure of how far from the regression line data points are. Residuals are nothing but prediction error, we can find it by subtracting the predicted value from actual value. In order to get RMSE we will use Standard deviation formula but instead of square root of variance we will calculate the square root of average of squared residuals. Standard deviation is used to measure the spread of data around the mean, while RMSE is used to measure distance between predicted and actual values. RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable. Note that, as per competition rules submissions are evaluated on the logarithm of the predicted value and the logarithm of the observed sales price. Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally. What does RMSE indicate?: It indicates the absolute fit of the model to the data. Provides average model prediction error in units of the variable of interest. They are negatively-oriented scores, which means lower values are better. 5.3.2 Correlation Check Correlation is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other. The logic behind using correlation for feature selection is that the good variables are highly correlated with the target. A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. We will use Pearsons correlation. It is utilized we you have two quantitative variables and you wish to see if there is a linear relationship between those variables. FIGURE 5.4: Correlation Map of the variables. In above correlation map: -1 indicates a perfectly negative linear correlation between two variables. 0 indicates no linear correlation between two variables. 1 indicates a perfectly positive linear correlation between two variables. 5.3.3 Multicollinearity Analysis Multicollinearity refers to a situation in which more than two explanatory variables in a multiple regression model are highly linearly related. Below are few of the multicollinear features based on correlation matrix. FIGURE 5.5: Correlation of Price vs dist subway, dist bus, dist school and live area. FIGURE 5.5: Boxplot of correlation of Price vs numbers Bathrooms, Bedrooms and Vacancies 5.3.4 Feature Engineering 5.3.4.1 Numeric Feature Scaling In order to give every feature the same importance we perform feature scaling. There are many techniques like Min-Max Scaler, Robust Scaler etc. to do feature scaling. Before we can finalize any scaling technique lets check the skewness of our numeric features. Skewness is the measure of degree of asymmetry of a distribution: skewness = 0 : normally distributed. skewness &gt; 0 : more weight in the left tail of the distribution. skewness &lt; 0 : more weight in the right tail of the distribution. 5.3.4.2 Target Variable Analysis and Transformation Price is our target variable. If we want to predict the target variables accurately then the first task is to understand the underlying behavior of our target variable. Note that model can make more reliable predictions if our target variable is normally distributed. We will use both graphical and statistical methods to do a normality test of our target variable. Statistical We will use Skewness, Kurtosis and Shapiro-Wilk test for normality. Skewness assesses the extent to which a variables distribution is symmetrical. The thumb rule is, If the skewness is between -0.5 and 0.5, the data are fairly symmetrical. If the skewness is between -1 and  0.5 or between 0.5 and 1, the data are moderately skewed. If the skewness is less than -1 or greater than 1, the data are highly skewed. Kurtosis tells you the height and sharpness of the central peak, relative to that of a standard bell curve. The thumb rule is, Kurtosis greater than +1 indicates distribution is too peaked. Kurtosis less than 1 indicates a distribution that is too flat. FIGURE 5.6: Rent Price Normal Distribution From a normal distribution plot its clear that the mean is greater than mode, similarly from the probability plot we can see that most of the observations fall on the lower end of the Y axis. So we can conclude that that the target variable Price is right skewed. There are multiple transformation techniques to handle the skewed data. We use log transformation. FIGURE 5.6: Rent Price Normal Distribution after apply Log 5.4 Prediction 5.4.1 Linear Models 5.4.1.1 Linear Regression Linear Regression fits a linear model with coefficients w = (w1, , wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. TABLE 5.2: Results for Random Forest with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3829 ± 0.0050 0.3798 5.4.1.2 Bayesian Ridge Regression Bayesian Ridge estimates a probabilistic model of the regression. Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand. The advantages of Bayesian Regression are: It adapts to the data at hand. It can be used to include regularization parameters in the estimation procedure. The disadvantages of Bayesian regression include: Inference of the model can be time consuming. TABLE 5.3: Results for Random Forest with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3829 ± 0.0050 0.3797 5.4.1.3 Lasso Linear Model trained with L1 prior as regularizer. TABLE 5.4: Results for Lasso with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3831 ± 0.0050 0.3801 5.4.1.4 Elastic Net Linear regression with combined L1 and L2 priors as regularizer. TABLE 5.5: Results for Elastic Net with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.4064 ± 0.0031 0.4035 5.4.1.5 Ridge Linear least squares with l2 regularization. This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets)). TABLE 5.6: Results for Ridge with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.4064 ± 0.0031 0.4035 5.4.1.6 Support Vector Machines Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target. TABLE 5.7: Results for Support Vector Machines with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3461 ± 0.0064 0.3427 5.4.2 Ensemble Methods The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator. Two families of ensemble methods are usually distinguished: In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced. Examples: Bagging methods, Forests of randomized trees,etc By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble. Examples: AdaBoost, Gradient Tree Boosting, etc 5.4.2.1 Gradient Boosting Regressor GBR supports a number of different loss functions for regression which can be specified via the argument loss; the default loss function for regression is least squares (ls). TABLE 5.8: Results for Gradient Boosting with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3148 ± 0.0050 0.3128 5.4.2.2 LightGBM Regressor (Light Gradient Boosting Machine) LightGBM is a gradient boosting framework based on decision trees to increases the efficiency of the model and reduces memory usage. It uses two novel techniques: Gradient-based One Side Sampling and Exclusive Feature Bundling (EFB) which fulfills the limitations of histogram-based algorithm that is primarily used in all GBDT (Gradient Boosting Decision Tree) frameworks. TABLE 5.9: Results for LightGBM with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3152 ± 0.0038 0.3198 5.4.2.3 XGB Regressor XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. XGBoost stands for Extreme Gradient Boosting We can use it to solve classification and regression problems. The XGBoost is a popular supervised machine learning model with characteristics like computation speed, parallelization, and performance. TABLE 5.10: Results for XGBoost with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3164 ± 0.0040 0.3124 5.4.3 Model Stacking 5.4.3.1 Stacking CV Regressor An ensemble-learning meta-regressor for stacking regression. Stacking is an ensemble learning technique to combine multiple regression models via a meta-regression. The StackingCVRegressor extends the standard stacking algorithm (implemented as StackingRegressor) using out-of-fold predictions to prepare the input data for the level-2 regressor. In the standard stacking procedure, the first-level regressors are fit to the same training set that is used to prepare the inputs for the second-level regressor, which may lead to overfitting. The StackingCVRegressor, however, uses the concept of out-of-fold predictions: the dataset is split into k folds, and in k successive rounds, k-1 folds are used to fit the first level regressor. In each round, the first-level regressors are then applied to the remaining 1 subset that was not used for model fitting in each iteration. The resulting predictions are then stacked and provided  as input data  to the second-level regressor. After the training of the StackingCVRegressor, the first-level regressors are fit to the entire dataset for optimal predictions. Ref. http://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/ FIGURE 5.7: Staking Model TABLE 5.11: Results for Stacking CV Regressor with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3112 ± 0.0048 0.3198 5.4.3.2 Model Blending And finally We use Model Blending to combine all models together. TABLE 5.12: Results for Model Blending with 5-Fold Cross-Validation. RMSLE on Training RMSLE on Test 0.2973 0.2923 5.4.4 Summary TABLE 5.13: Summary all models Model RMSLE on Training RMSLE on Test Linear Regression 0.38291 ± 0.00506 0.37982 Bayesian Ridge 0.3829 ± 0.0050 0.3797 Lasso 0.3831 ± 0.0050 0.3801 Elastic Net 0.4064 ± 0.0031 0.4035 Ridge 0.4064 ± 0.0031 0.4035 Support Vector Machine 0.3461 ± 0.0064 0.3427 Gradient Bossting 0.3148 ± 0.0050 0.3128 LightGBM 0.3152 ± 0.0038 0.3198 XGBoot 0.3164 ± 0.0040 0.3124 Stacing CV 0.3112 ± 0.0048 0.3198 Bleding 0.2973 0.2923 5.5 Use Case of the Model Having the model trained, tested and explained, it is time for the use cases. The first application can be someone who wants to rent a house and is looking for more attractive values. With the model it is possible to search for regions of interest and find out if the rental amount that the owner is charging is worth or not. In addition, it is possible to search for properties by price ranges. The second application is for the owner who wanted to rent his property. Knowing the value of the region he is in, he can make the value of his rent much more competitive against the competition of people who are also selected for renting. In addition, he can estimate with variables what is possible to add to his property or as options in the region to increase the value. 5.6 Conclusion Predicting real estate rent prices from online advertisements in Brazil is a task which requires insight into the data combined with powerful ML algorithms. Many locations in the city of São Paulo do not have standards in the choice of price, which makes it very difficult for machine learning models. In this work, we applied 9 different methods for this task, and combined them into a final prediction. We provided a strong baseline to overcome, as our final ensemble hit a score of 0.2973 RMSLE showing how powerful the combined model methods are. For future work, we can make a web scraper again on the site, take images of the properties and apply a covolutional neural network, an aggression that could help in the accuracy of the model with real photos inside the properties. "]]
