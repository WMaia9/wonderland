[["index.html", "Data Science Portfolio A Showcase of my Projects and my abilities Preface", " Data Science Portfolio A Showcase of my Projects and my abilities Wesley Maia 2022-10-27 Preface In this portfolio, you will find case studies covering the area of Data Science. "],["introduction.html", "Introduction Case studies", " Introduction The demand for skilled data science practitioners in industry, academia, and the government is rapidly growing. This portfolio introduces concepts and skills that can help it tackle real-world data analysis challenges. It covers probability, statistical inference, linear regression, and machine learning concepts. Case studies Throughout the portfolio, we use motivating case studies. We try to realistically mimic a data scientists experience in each case study. We start by asking specific questions and answering these through data analysis for each of the concepts covered. We learn the ideas as a means to answer the questions. Case studies included in the portfolio are: Case Study Concept GDP And Energy Supply Data Cleaning and Data Visualization Medical Insurance Costs Data Visualization and Linear Regression Quantium Retail Strategy And Analytics Statistical Inference Ok Cupid Descriptive statistics and Machine Learning Real Estate São Paulo Rent Prediction Complete Data Science Problem All codes and projects in this portfolio can be found on my Github page: GitHub "],["author.html", "About Me", " About Me Im a data scientist currently working at the Hand Talk company, which applies artificial intelligence for accessibility. I graduated in Science Physics, and at this moment, Im studying an MBA course in Data Science and Analytics. Im passionate about technology and science. At all times, I am always looking to learn new things. During graduation, I participated in Scientific Initiation projects. The first one, Improvement of collimation systems for gamma-particle coincidence measurements, consisted of calculating Rutherford collisions utilizing programming in C language and whose objective was to understand the operation of the collimator of the accelerator Pelletron from the University of São Paulo. The second one focused on the High Energy Physics area at LHC at CERN with case studies. I worked in the financial market for a while, interning at the trading desk. After that, I dedicated myself for a few years to teaching Physics. On this page, you will find some of the projects I am currently working on. You can see my Linkedin. Here. "],["gpd-and-energy-supply.html", "Project 1 GPD And Energy Supply 1.1 Introduction 1.2 Data 1.3 Data Analysis 1.4 Conclusion", " Project 1 GPD And Energy Supply Code: GitHub 1.1 Introduction GDP is an acronym that means Gross Domestic Product and it is something extremely important for nations, because it consists of the sum of a countrys monetary values. In this way, this means that all goods and other types of services that are produced in a given region are added together over a period of time to know whether they are growing or not. Thus, GDP is one of the most used indicators in the market economy, always aiming to carry out research on how all the economic activity of a country is doing. Equally important, measuring sustainability indicators allows plants to assess the positive impacts of power generation, as well as the economic, social and environmental gains provided by the project. Experts say combating climate change should be one of the pillars for post-pandemic economic recovery. This is due to the fact that sustainable investments can generate higher returns for the economy. In this sense, stimulating the generation of renewable energy in the plants is essential, both to reduce impacts on the environment and also for the plant to remain competitive even in challenging scenarios. And to assess this, it becomes even more important to monitor sustainability indicators on a daily basis. Thinking about the great importance that energy indicators and their research impact on GDP and the great world movements that are happening in the search for clean and renewable energy, it seems interesting to make a case study on this content. In this case study, some indicators of the last 10 years of the 15 largest economies in the world are analyzed. To do this, 3 datasets are used. A greater focus will be given to the analysis of Brazil in relation to other countries. The analysis is done using python with Jupyter Notebook. 1.2 Data 1.2.1 Energy Indicators Energy_Indicators.xls is a list of United Nations energy supply and renewable electricity production indicators for the year 2015 with the last update in 2017. Energy Indicators TABLE 1.1: Description of variables in the Energy Indicators dataset. Variable Description Country List of country Energy Supply (Petajoules) Energy supply in each country Energy Supply per Capita (Gigajoules) Energy supply per capita in each country % Renewable (%) Rate of renewable energy in the country 1.2.2 Country GDP world_bank.csv is a GDP data for countries between 1960 and 2019 (last updated) provided by the World Bank TABLE 1.2: Description of variables in the GDP dataset. Variable Description Country List of country Year (1960 until 2019) GDP 1.2.3 Energy Technology scimagojr-3.xlsx is a Engineering and Energy Technology data made available by the Scimago Journal &amp; Country Rank that are made available to the public and include journals and scientific indicators from developed countries based on the information contained in the Scopus database and that measure the impact and the influence of scientific publications, with the latest update being made available in 2019. Engineering and Energy Technology TABLE 1.3: Description of variables in the Country Rank data for Energy Engineering and Power Technology dataset. Variable Description Rank Rank of the country who have more citations Country Countries Country Documents Document numbers Citable documents Number of citable documents published by a journal in the three previous years. Citations Number of citations received in the selected year by a journal to the documents published in the three previous years. Self-citations Number of journals self-citations in the selected year to its own documents published in the three previous years. Citations per document Average citations per document in a 2 year period. H index journals number of articles (h) that have received at least h citations. Knowing which dataset we are going to work with, we can now look at them to get a better idea of how to proceed with the data. We start by looking at the first 5 lines of Energy Indicators TABLE 1.4: first 5 line of Energy Indicators. Country Energy Supply Energy Supply per Capita % Renewable Afghanistan 3.21e+08 10 78.6693 Albania 1.02e+08 35 100 Algeria 1.959e+09 51 0.55101 American Samoa nan  0.641026 Andorra 9e+06 121 88.6957 We can see that there is data that needs to be cleaned up. In these first lines, it is noted that Afghanistan and Andorra use a very high percentage of renewable energy. Next, well look at the GDP indicators TABLE 1.5: first 5 line of GDP Indicators. Country 2010 2011 2012  2018 2019 Aruba 2.3905e+09 2.54972e+09 2.53464e+09  nan nan Afghanistan 1.58566e+10 1.78043e+10 2.00016e+10  1.83539e+10 1.92911e+10 Angola 8.37995e+10 1.1179e+11 1.28053e+11  1.01353e+11 8.88157e+10 Albania 1.19269e+10 1.28908e+10 1.23198e+10  1.5147e+10 1.52792e+10 Andorra 3.44997e+09 3.6292e+09 3.18881e+09  3.21832e+09 3.15406e+09 Again we see some countries with values that are missing. I believe that it will not be a problem when the data was merged. Now lets analyze the data from the Scimago Journal and Country Rank data for Energy Engineering and Power Technology TABLE 1.6: first 5 line of the Country Rank data for Energy Engineering and Power Technology. Rank Country Region Documents Citable documents Citations Self-citations Citations per document H index 1 China Asiatic Region 235126 233883 1909601 1306438 8.12 224 2 United States Northern America 157811 154288 1940563 639345 12.3 333 3 Japan Asiatic Region 46032 45559 436961 109968 9.49 181 4 India Asiatic Region 39893 38848 368175 123446 9.23 171 5 United Kingdom Western Europe 38873 37780 536378 100038 13.8 208 After clearing all the data and renaming the country names so as not to duplicate the lines, we can immerse the data according to the first 15 placed in the ranking provided by Scimago Journal. We can see this table below. TABLE 1.7: All top Rank 15 Scimago Journal Countries. Country Rank Documents Citations Energy Supply % Renewable  2019 China 1 235126 1909601 1.27191e+11 19.7549  1.43429e+13 United States 2 157811 1940563 9.0838e+10 11.571  2.14332e+13 Japan 3 46032 436961 1.8984e+10 10.2328  5.08177e+12 India 4 39893 368175 3.3195e+10 14.9691  2.86893e+12 United Kingdom 5 38873 536378 7.92e+09 10.6005  2.82911e+12 Germany 6 32935 367356 1.3261e+10 17.9015  3.86112e+12 Russian Federation 7 31880 91906 3.0709e+10 17.2887  1.69988e+12 Canada 8 29633 491467 1.0431e+10 61.9454  1.73643e+12 Italy 9 23725 312631 6.53e+09 33.6672  2.00358e+12 South Korea 10 23451 279709 1.1007e+10 2.27935  1.64674e+12 France 11 22429 300015 1.0597e+10 17.0203  2.71552e+12 Iran 12 19371 242250 9.172e+09 5.70772  nan Spain 13 18882 312632 4.923e+09 37.9686  1.39349e+12 Australia 14 18077 263733 5.386e+09 11.8108  1.39657e+12 Brazil 15 18024 152380 1.2149e+10 69.648  1.83976e+12 After immersing the data, a 15-row, 20-column Data Frame was created. We can see the columns partially due to the limitation of the page, it can be seen in full how the immersion process was done and with all the data on the GitHub page 1.3 Data Analysis Now that we have cleaned up and merged the data, we can make inferences. 1.3.1 What are the top 15 countries for average GDP over the past 10 years? FIGURE 1.1: The distribution of the top 15 GDP country. As can be seen in the graph, in the average GDP of the last 10 years there is a sovereignty of the USA still in relation to the other countries with China and Japan just behind the top 3. Brazil is the 8th best GDP average among the 15 largest. 1.3.2 How much did the average GDP vary over the 10-year period for Brazil? GDP has changed by -369.0 billion dollars over the past 10 years FIGURE 1.2: Distribution of the variation in Brazil GPD over the last 10 years. As can be seen in the information above, Brazil over the past 10 years has had a negative change in GDP. The graph above shows that the biggest drop occurred between 2015 and 2016, which can be explained by the national political-economic crisis, where sets of economic measures, in addition to certain external factors and certain internal political events, which occurred during the Dilma Rousseff Government, which, added , resulted in a drop in activity. In 2020 another recession is expected due to the global financial crisis caused by the Covid-19 pandemic. 1.3.3 What is the average energy supply per capita? The average energy supply is 157.6 Petajoules 1.3.4 What percentage do countries use of Renewable Energy? FIGURE 1.3: The distribution of the percentage fo countries use Renewable Energy. Analyzing the chart above, we see that Brazil is the country that has the highest percentage of use of renewable energy. According to the International Energy Agency, Brazil is the third largest renewable energy generator in the world and according to the Energy Research Company (EPE), the country has a predominant renewable energy electrical matrix, with emphasis on the hydraulic generation which accounts for 68.1% of the domestic supplyand is one of the great Brazilian forces when it comes to renewable energy, thus justifying this high percentage. 1.3.5 What is the estimated population size using Energy Supply and Energy Supply per capita? FIGURE 1.4: The distribution of the Estimated Population in Billions of people. As the data were not available due to the size of the population, a simple way is to use the Energy Supply and Energy Supply values per capita to make an estimate. Doing a quick search on the internet is easy to see that the figures are very close to the real estimate of the population of these countries. We see that China has almost 1.4 billion people, followed by India with 1.3 billion and then the USA with approximately 300 million. 1.3.6 What is the proportion of self-citations to the total citations? FIGURE 1.5: The distribution of the rank of the country with more self citations. Analyzing now more data related to Scimago Journal Rank, which is a measure of the scientific influence of academic journals that accounts for the number of citations received for a period and the importance or prestige of the journals from which these situations come, we see that China is the country which has a higher proportion of self-quotes on Energy and Telecommunications articles showing how interested China has been in this area in recent years. In Latin America, this impact is very visible with the recent revolution in the electricity transmission and distribution sector with the arrival of the Chinese state conglomerates State Grid and Three Gorges in the markets of Chile and Peru and which continue to expand their business in Brazil. 1.3.7 What is the correlation between the number of documents cited per capita and the supply of energy per capita? An interesting analysis that can be seen with the data provided is whether there is a correlation between the number of documents cited per capita and the energy supply and we can see that there is a great relationship between these aspects what it is 0.75. We see then that the greater the interest of a country in studies related to Energy, the greater is also the energy supply of that same country. 1.4 Conclusion After analyzing the dataset separately, we were able to use one of the great advantages of python to analyze data is the joining of data. With Datasets together we were able to make interesting inferences about the data. From GPDs at home in two countries to the correlation that there is energy supply and the number of citations visualizing graphs and data, showing the great power that the Python language has in data analysis and proving to be a great tool for Data Science. "],["medical-insurance-costs.html", "Project 2 Medical Insurance Costs 2.1 Introduction 2.2 Data 2.3 Exploring the Data 2.4 Linear Regression 2.5 Visualization of the fitted models 2.6 Conclusion", " Project 2 Medical Insurance Costs Code: GitHub 2.1 Introduction There is no doubt that health-related benefits are among the most important that a company can offer its employees. After all, they help significantly reduce medical expenses and guarantee the necessary support in times of difficulty. All of this reflects on the collective well-being and brings positive results. Among the significant advantages of hiring health insurance in organizations, we can mention the greater engagement of the team. This is because employees who access quality medical care work with more attention, tranquility, and disposition, achieving their best performance during working hours. Because Medical insurance is a much-desired benefit, its offer also helps attract and retain the best professionals in the market. Moreover, with great talents working in the company, it tends to become a benchmark for quality in what it does. In this project, we analyze data from US Medical Insurance Costs. Recently, many people have been looking for this type of insurance, and prices vary widely according to the users data. The main question of this analysis is to answer the final price that users need to pay. The dataset insurance.csv was provided by Kaggle.com: Medical Cost Personal Datasets 2.1.1 Project Goals This project is making a case study using the linear regression method on the Medical Insurance Costs dataset provided by the Kaggle. 2.1.2 Analysis This solution uses descriptive and inferential statistics and data visualization to find key figures in understanding the distribution, count, and relationship between variables. 2.1.3 Evaluation The project concludes with the Evaluation of a linear model. A simple case study is done for one variable, and multiple variables are focused on visualizing the adjusted models then looking at the residual plots and the error band. 2.2 Data The project has one dataset provided by Kaggle called insurance.csv. Each row represents a user in the data, and the columns are the responses to their user profiles, including data about them. insurance has 1,338 rows and 7 columns, this is a good amount of data to analyze. Each line, or sample, consists of the following features: TABLE 2.1: Description of variables in the dataset. Variable Description age Age of primary beneficiary sex Clients gender bmi Body mass index of the client. BMI is a measure of a persons weight with respect to their height. children Numbers of children depending on the client smoker smoker yes or no region Region of which the customer is a part charges Annual charge of how much the customer must pay 2.3 Exploring the Data Now let us look at the first five lines of the Data Frame that we want to analyze. TABLE 2.2: The 5 first lines of the Data Frame Insurance. age sex bmi children smoker region charges 19 female 27.9 0 yes southwest 16884.9 18 male 33.77 1 no southeast 1725.55 28 male 33 3 no southeast 4449.46 33 male 22.705 0 no northwest 21984.5 32 male 28.88 0 no northwest 3866.86 Before doing a deeper analysis of the data, we can see some of the data and how it is distributed from graphs. We started by looking at the Age Histogram. FIGURE 2.1: The distribution of the Age Histogram. Then, we can look at the scatter graph in the age variable x charge and separate between smokers and non-smokers, as we believe it has a significant impact on the value of the final price charged. FIGURE 2.2: Scatter plot of chargers vs age between smokers and non-smokers. No-Smoker = 0 and Smoker = 1 As can be seen in FIGURE 2.2, we have a leap in value when we analyze the graphs between smokers and non-smokers. Furthermore, we can see that there is a linear growth between age and the amount charged. Next, let us look at the BMI distribution: FIGURE 2.3: The BMI distribution. The BMI values are all well distributed in Gaussian. We see that there are no outliers to be limited. Knowing that BMI has a well-behaved distribution, we will look at how BMI is influenced by the processed value between treatment and untreated. FFIGURE 2.4: Scatter plot of charges vs. BMI between smokers and non-smokers. Analyzing FIGURE 2.1, we can see a linear growth between BMI and the amount charged when smokers, which is not the case for non-smokers. Taking a first look at the data provided and some plotted graphs, we can see that there are probably relationships between some variables provided and the charge amount. Next, we will investigate it. 2.4 Linear Regression We initially focus on regression models where the amount charged is the dependent variable. That is, we predict the amount charged based on other variables. Since the feature charge is a quantitative variable, we model it using linear regression. 2.4.1 Interpreting regression parameters in a basic model We start with a simple linear regression model with only one covariable, age, to predict the charge. The adjusted model expresses the amount charged as a linear function of age. TABLE 2.3: Linear Regression Model adjusted for age variable using the python statsmodels library. The part that starts to be relevant to the Table presented is from coef. This section contains the estimated values for the parameters of the regression model. Its pattern, errors, and other values are used to quantify the uncertainty in the estimates of the regression parameters. This adjusted model implies that when comparing two people of different ages in one year, the oldest person will have an average of 257.72 units of charges larger than the youngest person. This difference is statistically significant, based on the p-value shown in the column labeled. P&gt; | t |. This means that there is strong evidence that there is a natural association between age and charges made by the customer. To better understand the meaning of the regression parameter 257.7226, we can look at the standard deviation of charges. The standard deviation of around $ 12,110 is significant compared to the slope of the 258 regression. However, the regression slope corresponds to the average change in collection for a single year of age, and this effect accumulates with age. Comparing a 40 year old person with a 60 year old person, there is a 20-year difference, which translates into 20 * 258 = 5,160 unit difference in the average value charged between these two people. This difference is about less than half a standard deviation and is generally considered to be a significant change. 2.4.2 R-square In the case of regression with a single independent variable, as we have here, there is a very close correspondence between the regression analysis and Pearsons correlation analysis. The R-square is a statistical measure of how close the data is to the adjusted regression line. The value obtained was 0.09, which means that the age of the customers explains 9% of the variation in the amount charged. 2.4.3 Adding more variables to the model We use all the variables after analyzing a simple linear regression model with only one covariant (age). The real power of regression happens when we have more than one covariant to predict an output. TABLE 2.4: Linear Regression Model adjusted for all variables using the python statsmodels library. The model adjusted in TABLE 2.4 uses all parameters to explain the variation in the amount charged. We can see how attractive the value of R-square is. If we look at the table, we see a value of 0.75. That is, we have a linear regression model that 75% of the variation in the amount charged is probably explained by the variables provided. Looking at the age coefficient, which we studied earlier, there were no changes, even adding many other variables. Thus, we can notice that the age variable has no relation with the other variables provided in the model. It is possible to confirm this result by looking at the value of the correlations between the variables and noting that there is almost no relationship between age and other variables. FIGURE 2.5: Correlation between variables 2.5 Visualization of the fitted models In this section, we demonstrate some graphing techniques that can be used to understand better a regression model that has been fit to data. We start with plots that allow us to visualize the fitted regression function: the mean charges expressed as a function of the covariates. These plots help to show the estimated role of one variable when the other variables are held fixed. We will also plot 95% simultaneous confidence bands around these fitted lines. Although the estimated mean curve is never exact based on a finite sample of data, we can be 95% confident that the true mean curve falls somewhere within the shaded regions of the plots below. This type of plot requires us to fix the values of all variables other than the independent variable (Charges here) and one independent variable that we call the focus variable (which is age here). Below we fix the gender as female and the BMI as 25. Thus, the graphs below show the relationship between expected Charges and age for women with BMI equal to 25. The plot analyzed for the number of children is shown below. Age is 50, with the other parameters the same. FIGURE 2.6: Fitted regression function charge vs age. The analogous plot for Number of Children is shown next. Here we fix the gender as female and the age at 50, so we are looking at the relationship between common charge and age for women of 50. FIGURE 2.7: Fitted regression function charge vs number of children. The error band for the number of children is noticeably more significant than the error band for the age, indicating less certainty about children and charge than age and charge. The discussion has primarily focused on the mean structure of the population, that is, the model for the average change of a person with a given age, gender, BMI, smoker, region, and children. A regression model can also be used to assess the variance structure of the population, that is, how much and in what manner the observations deviate from their mean. We will focus on informal, graphical methods for assessing this. To begin with, we plot the residuals against the fitted values. Recall that the fitted values are the estimated means for each observation, and the residuals are the difference between an observation and its fitted mean. For example, the model may estimate that a 50-year-old female will have, on average, a charge of 32,000 $. However, a specific 50-year-old female may have a blood pressure of 110 or 150, for example. The fitted values for these women are 32,000, and their residuals are -10,000 and 10,000, respectively. The most straightforward variance pattern that we can see in a linear regression occurs when the points are scattered around the mean, with the same degree of scattering throughout the range of the covariates. When there are multiple covariates, it is hard to assess whether the variance is uniform throughout this range. However, we can quickly check for a mean/variance relationship, in which there is a systematic relationship between the variance and the mean, i.e., the variance either increases or decreases systematically with the mean. The plot of residuals on fitted values is used to assess whether such a mean/variance relationship is present. FIGURE 2.8: Fitted values vs Residuals. Above we show the plot of residuals on fitted values for the Insurance data. It appears that we have a modestly increasing mean/variance relationship. 2.6 Conclusion As can be seen, the linear regression model is a potent tool for analyzing model predictions if more than one variable is used. Using the python statsmodels library, we saw how easy and beneficial it is to analyze statistical models. "],["quantium-retail-strategy-and-analytics.html", "Project 3 Quantium Retail Strategy and Analytics 3.1 Introduction 3.2 Data 3.3 Exploring the Data 3.4 Data analysis on customer segments 3.5 Deep dive into specific customer segments for insights 3.6 Conclusion", " Project 3 Quantium Retail Strategy and Analytics Code: GitHub 3.1 Introduction This work is part of the virtual internship program of the company Quantium. Quantium is a Data Science company that helps companies with insights and models looking to improve their performance. Quantium has had a data partnership with a major supermarket brand in recent years, which provides transactional and customer data. The analysis is based on Quantium chip data to better understand the types of customers who buy chips and their buying behavior in the region. The analysis insights serve to feed the supermarkets strategic plan for the chip category. 3.2 Data The first step in any analysis is to first understand the data. Lets take a look at each of the datasets provided. 3.3 Exploring the Data Starting with the analysis of the dataset transaction, lets look at the first few lines. 3.3.1 Examining transaction data DATE STORE LYLTY_CARD TXN_ID PROD NAME QTY TOT_SALES 43390 1 1000 1 5 Natural Chip Compny SeaSalt175g 2 6 43599 1 1307 348 66 CCs Nacho Cheese 175g 3 6.3 43605 1 1343 383 61 Smiths Crinkle Cut Chips Chicken 170g 2 2.9 43329 2 2373 974 69 Smiths Chip Thinly S/Cream&amp;Onion 175g 5 15 43330 2 2426 1038 108 Kettle Tortilla ChpsHny&amp;Jlpno Chili 150g 3 13.8 As we are only interested in words that tell us if the product is chips or not, lets remove all words with digits and special characters such as &amp; from our set of product words. There are salsa products in the dataset but we are only interested in the chips category, so lets remove these. Next, we can check summary statistics such as mean, min and max values for each feature to see if there are any obvious outliers in the data and if there are any nulls in any of the columns STORE QTY TOT_SALES PROD_SIZE count 246740 246740 246740 246740 mean 135.05 1.90646 7.31611 175.584 std 76.787 0.342499 2.4749 59.4321 min 1 1 1.7 70 25% 70 2 5.8 150 50% 130 2 7.4 170 75% 203 2 8.8 175 max 272 5 29.5 380 There are no nulls in the columns but product quantity appears to have an outlier which we should investigate further. Lets investigate further the case where 200 packets of chips are bought in one transactions. DATE STORE LYLTY_CARD TXN_ID PROD PROD_NAME QTY TOT_SALES PROD_SIZE 2018-08-19 226 226000 226201 4 dorito corn chp supreme 200 650 380 2019-05-20 226 226000 226210 4 dorito corn chp supreme 200 650 380 There are two transactions where 200 packets of chips are bought in one transaction and both of these transactions were by the same customer. Lets see if the customer has had other transactions DATE STORE LYLTY_CARD TXN_ID PROD NAME PROD_QTY TOT_SALES PROD_SIZE 2018-08-19 226 226000 226201 4 dorito corn chp supreme 200 650 380 2019-05-20 226 226000 226210 4 dorito corn chp supreme 200 650 380 It looks like this customer has only had the two transactions over the year and is not an ordinary retail customer. The customer might be buying chips for commercial purposes instead. We remove this loyalty card number from further analysis. STORE QTY TOT_SALES PROD_SIZE count 246740 246740 246740 246740 mean 135.05 1.90646 7.31611 175.584 std 76.787 0.342499 2.4749 59.4321 min 1 1 1.7 70 25% 70 2 5.8 150 50% 130 2 7.4 170 75% 203 2 8.8 175 max 272 5 29.5 380 Thats better. Now,looking at the number of transaction lines over time to see if there are any obvious data issues such as missing data, we see that is missing a value. Theres only 364 rows, meaning only 364 dates which indicates a missing date. figure 1 Zooming in on the data to take a closer look: We can see that the increase in sales occurs in the lead-up to Christmas and that there are zero sales on Christmas day itself. This is due to shops being closed on Christmas day. Now that we are satisfied that the data no longer has outliers, we can move on to creating other features such as brand of chips or pack size from PROD_NAME. We start with pack size. The largest size is 380g and the smallest size is 70g - seems sensible! Lets plot a histogram of PACK_SIZE since we know that it is a categorical variable and not a continuous variable even though it is numeric. figure3 3.3.2 Examining customer data Now that we are happy with the transaction dataset, lets have a look at the customer dataset. LYLTY_CARD_NBR LIFESTAGE PREMIUM_CUSTOMER 1000 YOUNG SINGLES/COUPLES Premium 1002 YOUNG SINGLES/COUPLES Mainstream 1003 YOUNG FAMILIES Budget 1004 OLDER SINGLES/COUPLES Mainstream 1005 MIDAGE SINGLES/COUPLES Mainstream We will now join the two data using the python merge function. DATE LYLTY_CARD_NBR TXN_ID PROD_NBR PROD_NAME QTY TOT_SALES SIZE LIFESTAGE PREMIUM_CUSTOMER 2018-10-17 1000 1 5 natural chip compny seasalt 2 6 175 YOUNG SINGLES/COUPLES Premium 2019-05-14 1307 348 66 ccs nacho cheese 3 6.3 175 MIDAGE SINGLES/COUPLES Budget 2019-05-20 1343 383 61 smiths crinkle cut chips chicken 2 2.9 170 MIDAGE SINGLES/COUPLES Budget 2018-08-17 2373 974 69 smiths chip thinly cream onion 5 15 175 MIDAGE SINGLES/COUPLES Budget 2018-08-18 2426 1038 108 kettle tortilla chpshny jlpno chili 3 13.8 150 MIDAGE SINGLES/COUPLES Budget As the number of rows in result is the same as that of transactionData, we can be sure that no duplicates were created. This is because we created result by setting a left join which means take all the rows in transactionData and find rows with matching values in shared columns and then join the details in these rows to the x or the first mentioned table. 3.4 Data analysis on customer segments Now that the data is ready for analysis, we can define some metrics of interest to the client: - Who spends the most on chips (total sales), describing customers by lifestage and how premium their general purchasing behaviour is - How many customers are in each segment - How many chips are bought per customer by segment - Whats the average chip price by customer segment Lets start with calculating total sales by LIFESTAGE and PREMIUM_CUSTOMER and plotting the split by these segments to describe which customer segment contributes most to chip sales. TOT_SALES PREMIUM_CUSTOMER LIFESTAGE Budget MIDAGE SINGLES/COUPLES 33345.70 NEW FAMILIES 20607.45 OLDER FAMILIES 156863.75 OLDER SINGLES/COUPLES 127833.60 RETIREES 105916.30 YOUNG FAMILIES 129717.95 YOUNG SINGLES/COUPLES 57122.10 Mainstream MIDAGE SINGLES/COUPLES 84734.25 NEW FAMILIES 15979.70 OLDER FAMILIES 96413.55 OLDER SINGLES/COUPLES 124648.50 RETIREES 145168.95 YOUNG FAMILIES 86338.25 YOUNG SINGLES/COUPLES 147582.20 Premium MIDAGE SINGLES/COUPLES 54443.85 NEW FAMILIES 10760.80 OLDER FAMILIES 75242.60 OLDER SINGLES/COUPLES 123537.55 RETIREES 91296.65 YOUNG FAMILIES 78571.70 YOUNG SINGLES/COUPLES 39052.30 Plotting the bar graph of the data, we have: Sales are coming mainly from Budget - older families, Mainstream - young singles/couples, and Mainstream - retirees Lets see if the higher sales are due to there being more customers who buy chips. TOT_SALES PREMIUM_CUSTOMER LIFESTAGE Budget MIDAGE SINGLES/COUPLES 35514.80 NEW FAMILIES 21928.45 OLDER FAMILIES 168363.25 OLDER SINGLES/COUPLES 136769.80 RETIREES 113147.80 YOUNG FAMILIES 139345.85 YOUNG SINGLES/COUPLES 61141.60 Mainstream MIDAGE SINGLES/COUPLES 90803.85 NEW FAMILIES 17013.90 OLDER FAMILIES 103445.55 OLDER SINGLES/COUPLES 133393.80 RETIREES 155677.05 YOUNG FAMILIES 92788.75 YOUNG SINGLES/COUPLES 157621.60 Premium MIDAGE SINGLES/COUPLES 58432.65 NEW FAMILIES 11491.10 OLDER FAMILIES 81958.40 OLDER SINGLES/COUPLES 132263.15 RETIREES 97646.05 YOUNG FAMILIES 84025.50 YOUNG SINGLES/COUPLES 41642.10 Plotting again: There are more Mainstream - young singles/couples and Mainstream - retirees who buy chips. This contributes to there being more sales to these customer segments but this is not a major driver for the Budget - Older families segment. Higher sales may also be driven by more units of chips being bought per customer. Lets have a look at this next. Mainstream mid aged and young singles and couples are more willing to pay more per packet of chips compared to their budget and premium counterparts. This may be due to premium shoppers being more likely to buy healthy snacks and when they buy chips, this is mainly for entertainment purposes rather than their own consumption. This is also supported by there being fewer premium mid aged and young singles and couples buying chips compared to their mainstream counterparts. As the difference in average price per unit isnt large, we can check if this difference is statistically different. The t-test results &lt; 2.2e-16, i.e. the unit price for mainstream, young and mid-age singles and couples are significantly higher than that of budget or premium, young and mid age singles and couples. 3.5 Deep dive into specific customer segments for insights We have found quite a few interesting insights that we can dive deeper into. We might want to target customer segments that contribute the most to sales to retain them or further increase sales. Lets look at Mainstream - young singles/couples. For instance, lets find out if they tend to buy a particular brand of chips. MIDAGE SINGLES/COUPLES smiths crinkle chips salt vinegar 194 cheezels cheese 186 doritos corn chips nacho cheese 179 kettle chilli 179 cobs popd sour crm chives chips 176 YOUNG SINGLES/COUPLES tostitos splash of lime 335 kettle mozzarella basil pesto 332 doritos corn chips cheese supreme 326 smiths crnkle chip orgnl big bag 323 kettle tortilla chpshny jlpno chili 323 We can see that : - Mainstream young singles/couples are 23% more likely to purchase Tyrrells chips compared to the rest of the population - Mainstream young singles/couples are 56% less likely to purchase Burger Rings compared to the rest of the population Lets also find out if our target segment tends to buy large packs of chips. MIDAGE SINGLES/COUPLES 175.0 2975 150.0 1777 134.0 1159 110.0 1124 170.0 882 MIDAGE SINGLES/COUPLES 175.0 2975 150.0 1777 134.0 1159 110.0 1124 170.0 882 Both the segment buy 175g, 150g and 134g packets mostly 3.6 Conclusion Sales have mainly been due to Budget - older families, Mainstream - young singles/couples, and Mainstream - retirees shoppers. We found that the high spend in chips for mainstream young singles/couples and retirees is due to there being more of them than other buyers. Mainstream, midage and young singles and couples are also more likely to pay more per packet of chips. This is indicative of impulse buying behavior. Weve also found that Mainstream young singles and couples are 23% more likely to purchase Tyrrells chips compared to the rest of the population. The Category Manager may want to increase the categorys performance by off-locating some Tyrrells and smaller packs of chips in discretionary space near segments where young singles and couples frequent more often to increase visibility and impulse behavior. Quantium can help the Category Manager with recommendations of where these segments are and further help them with measuring the impact of the changed placement. Well work on measuring the impact of trials in the next task and putting all these together in the third task. "],["ok-cupid.html", "Project 4 Ok Cupid 4.1 Introduction 4.2 Data 4.3 Exploring the Data 4.4 Data Preparation 4.5 Prediction 4.6 Conclusion", " Project 4 Ok Cupid Code: GitHub 4.1 Introduction This project analyzes data from on-line dating application OKCupid. In recent years, there has been a massive rise in the usage of dating apps to find love. Many of these apps use sophisticated data science techniques to recommend possible matches to users and to optimize the user experience. These apps give us access to a wealth of information that weve never had before about how different people experience romance. The dataset okcupid_profiles.csv was provided by Kaggle.com: OkCupid Profiles 4.1.1 Project Goals In this project, the goal is to analyze the data from Kaggle using tools of Data Science. The primary research question that will be answered is whether an OkCupids user astrological sign can be predicted using other variables from their profiles. 4.1.2 Analysis This solution uses descriptive statistics and data visualization to find key figures in understanding the distribution, count, and relationship between variables. Since the goal of the project is to make predictions on the users astrological signs, classification algorithms from the supervised learning family of machine learning models are implemented. 4.1.3 Evaluation The project concludes with the evaluation of the machine learning model selected with a validation data set. The output of the predictions can be checked through a confusion matrix, and metrics such as accuracy, precision, recall, F1 and Kappa scores. 4.2 Data The project has one data set provided by Kaggle called okcupid_profiles.csv. In the data, each row represents an OkCupid user and the columns are the responses to their user profiles which include multi-choice and short answer questions. To analyze the user profiles from OkCupid, pandas will be used to load the dataset into a DataFrame so that it can be explored and visualized with Python. 4.2.1 Data Characteristics profiles has 59,946 rows and 31 columns, this is a good sign since there seems to be enough data for machine learning. The columns in the dataset include: TABLE 4.1: Description of variables in the dataset. Variable Description age categorical variable of educational attainment body_type categorical variable of body type of user diet categorical variable of dietary information drinks categorical variable of alcohol consumption drugs categorical variable of drug usage education categorical variable of educational attainment ethnicity categorical variable of ethnic backgrounds height continuous variable of height of user income continuous variable of income of user job categorical variable of employment description offspring categorical variable of children status orientation categorical variable of sexual orientation pets: categorical variable of pet preferences religion categorical variable of religious background sex categorical variable of gender sign categorical variable of astrological symbol smokes categorical variable of smoking consumption speaks categorical variable of language spoken status categorical variable of relationship status last_online date variable of last login location categorical variable of user locations And a set of open short-answer responses to : TABLE 4.2: Description of variables short-answer in the dataset. Variable Description essay0 My self summary essay1 What Im doing with my life essay2 Im really good at essay3 The first thing people usually notice about me essay4 Favorite books, movies, show, music, and food essay5 The six things I could never do without essay6 I spend a lot of time thinking about essay7 On a typical Friday night I am essay8 The most private thing I am willing to admit essay9 You should message me if 4.3 Exploring the Data Lets start by looking at the first rows and columns of our dataset: TABLE 4.3: The 5 first lines of the Data Frame. age status sex orientation body_type drinks drugs education 22 single m straight a little extra socially never working on college/university 35 single m straight average often sometimes working on space camp 38 available m straight thin socially nan graduated from masters program 23 single m straight thin socially nan working on college/university 29 single m straight athletic socially never graduated from college/university First to be explored is the number of unique signs, and the values. It seems that there are 48, but there should only be 12 signs. It is important that we clean the labels since this is what will be predicted and 48 predictions would be quite difficult. By taking the first word of the column, the signs can be saved without the qualifiers. The qualifiers could be used for another problem down the line. After adjusting the signs, we can better analyze them on our dataset. 4.3.1 Continuous Variables 4.3.1.1 age The next plot shows the distribution of age in the group. It seems that most users are in their late 20s to early 30s. Here is the same chart but broken down by gender. It seems that there are proportionally similar breaks of gender by age, but slightly fewer females overall. 4.3.1.2 Height The next plot shows the height variable. most people look like they are between 1.5 and 2 meters tall. Here is the same height chart showing the breakdown by gender. It seems obvious, but females tend to be shorter than males and look to have a normal distribution. 4.3.1.3 Income Here is the data of income, it seems that the majority of the participants do not include their income figures. 4.3.2 Discrete Variables 4.3.2.1 Sex Previously it was identified that there are more males in the data, and it seems that there are ~35,000 men to ~25,000 women. 4.3.2.2 Body Type The next chart shows the body type variable, and it seems that most users will describe themselves as average, fit, or athletic. The next chart shows the breakdown of body type by gender and it seems that some of the body type descriptions are highly gendered. For example curvy and full figured are highly female descriptions, while males use a little extra, and overweight more often. 4.3.2.3 Diet Here is a chart of the dietary information for users. Most users eat mostly anything, followed by anything, and strictly anything, being open-minded seems to be a popular signal to potential partners. 4.3.2.4 Drinks The next plot shows that the majority of the users drink socially, then rarely and often. 4.3.2.5 Drugs The vast majority of users never use drugs. 4.3.2.6 Smoking Similarly for drugs the majority of users chose no for smoking. 4.3.2.7 Education Below you can see the majority of users are graduates from college/university followed by masters programs and those working on college/university. Interestingly space camp related options are fairly popular options. 4.3.2.8 Jobs Most users dont fit into the categories provided, but there are a fair share of students, artists, tech, and business folks. 4.3.2.9 Offspring The data suggest that most users do not have kids. 4.3.2.10 Orientation The majority of users are straight. 4.3.2.11 Pets The chart shows that most users like or have dogs. 4.3.2.12 Religion Religion was similar to sign where there are a lot of qualifiers. religion was cleaned to take the first word and distilled down to 9 groups. The majority was not very religious identifying as agnostic, other, or atheists. 4.3.2.13 Signs Here are the astrological signs of the users. They are mainly evenly distributed with Capricorns being the rarest and Leos being the most common. 4.3.2.14 Status The relationship status for a dating website is fairly predictable. One would assume that most people are single and available which is reflected in the data. 4.4 Data Preparation Missing data is often not handled by machine learning algorithms well and have to be checked so they may need to be imputed or removed. It seems that many of the columns do have missing values. Preparing the data for modeling is important since it can speed up the process and produce better models. As the adage goes, garbage in garbage out so we want to make sure the data we are inputting into our modelling step is good enough to share with others. The data for the model is going to be a subset of the variables. The variables were selected because they might be a good predictor for astrological signs, where some of the variables that were not selected such as age is probably not a good indicator. Missing values are dropped to create a fully complete data set. Furthermore, an imbalance in the prediction label needs to be checked. This is important since its a multi-class problem where two or more outcomes can be bad. An imbalance in a response variable is bad since it means that some labels only occur a few times. This is an issue for machine learning algorithms if there is not enough data to train with which will give bad predictions. In the given dataset, we observe that the counts of all the zodiac signs are more or less equal (i.e., without large deviations). Hence, we do not have to worry about imbalances and trying to address this problem. Next the data was split into train and validation sets. In this split 25% of the data is reserved for the final validation, while 75% is kept for training the model. 4.5 Prediction 4.5.1 Model building Now its time to create some models, here is a list of Multi class models available in scikit learn. For this project three common algorithms will be used to make predictions. Below, the respective modules for Logistic Regression, Decision Trees, and KNN are loaded. 4.5.2 Evaluation Metrics In the models, there will be several values that can be evaluated below is a quick diagram: here is a quick description of the metrics: Accuracy: is the correct values divided by total values Precision: is the True Positives divided by the sum of True Positives and False Negatives. So precision is the values of the true positives divided by the actual positive values. Recall: is the True Positives divided by the sum of True Positives and False Positives. So recall is the values of the true positives divided by the positive guesses. F1-score: is a blended score of precision and recall which balances both values. Macro Avg: is the unweighted mean value of precision and recall. Weighted Avg: is the weighted mean value of precision and recall by the support values for each class. Support: is the number of observations in class to predict. 4.5.3 Logistic Regression The first model is using logistic regression with the multi_class=\"multinomial\" argument. Using lr_model predictions are created from the training dataset which is used to figure out how well the model performed. precision recall f1-score support aquarius 0.13 0.06 0.08 930 aries 0.13 0.11 0.12 1012 cancer 0.13 0.20 0.15 1067 capricorn 0.14 0.05 0.08 921 gemini 0.12 0.12 0.12 1116 leo 0.12 0.18 0.15 1123 libra 0.13 0.12 0.13 992 pisces 0.12 0.11 0.12 1026 sagittarius 0.12 0.06 0.08 993 scorpio 0.12 0.08 0.09 1013 taurus 0.12 0.14 0.13 1051 virgo 0.13 0.21 0.16 1095 accuracy 0.12 12339 macro avg 0.12 0.12 0.12 12339 weighted avg 0.12 0.12 0.12 12339 The final accuracy of the logistic regression model is 12% which is terrible considering a random guess should result in being correct ~8% of the time (1/12). 4.5.4 K Nearest Neighbor The next model is the KNeighborsClassifier which will take 20 of its neighbors to predict the signs. The default value for n_neighbors is 5 which was kept. This number can be tuned later on if needed. precision recall f1-score support aquarius 0.26 0.66 0.37 930 aries 0.26 0.53 0.35 1012 cancer 0.30 0.44 0.35 1067 capricorn 0.34 0.37 0.35 921 gemini 0.34 0.32 0.33 1116 leo 0.40 0.28 0.33 1123 libra 0.38 0.23 0.29 992 pisces 0.42 0.25 0.31 1026 sagittarius 0.43 0.23 0.30 993 scorpio 0.42 0.22 0.29 1013 taurus 0.42 0.25 0.31 1051 virgo 0.42 0.23 0.29 1095 accuracy 0.33 12339 macro avg 0.36 0.33 0.32 12339 weighted avg 0.37 0.33 0.32 12339 This model had a 33% accuracy which is a good sign. 4.5.5 Decision Trees The last model is the decision tree, the default max_depth is none which means that it will If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.. precision recall f1-score support aquarius 0.66 0.94 0.78 930 aries 0.67 0.89 0.76 1012 cancer 0.70 0.87 0.78 1067 capricorn 0.73 0.84 0.78 921 gemini 0.76 0.79 0.78 1116 leo 0.80 0.79 0.80 1123 libra 0.79 0.76 0.78 992 pisces 0.81 0.74 0.77 1026 sagittarius 0.88 0.70 0.78 993 scorpio 0.89 0.71 0.79 1013 taurus 0.94 0.67 0.78 1051 virgo 0.91 0.67 0.77 1095 accuracy 0.78 12339 macro avg 0.80 0.78 0.78 12339 weighted avg 0.80 0.78 0.78 12339 The results are very promising because it has a 78% accuracy with this model. Below is a confusion matrix of the results with the true values on the y axis and predicted values along the x axis. Since the diagonals are lighter in color and have higher numbers, the accuracy is going to be high since those are the True Positives. Going back to the model, a quick analysis shows that this tree model has a depth of 65 branches, which probably not generalize to another dataset. In this case this model has been overfit for this data. To make a point, a five fold cross validation is created with the same data. The results are worse than the KNN and about the Logistic Regression algorithms. the baseline was ~9% The decision tree model will be made again, but with a max_depth of 20 to stop the algorithm from reaching the stopping point. precision recall f1-score support aquarius 0.40 0.33 0.36 930 aries 0.40 0.27 0.32 1012 cancer 0.40 0.28 0.33 1067 capricorn 0.27 0.30 0.28 921 gemini 0.37 0.26 0.31 1116 leo 0.37 0.25 0.30 1123 libra 0.59 0.19 0.29 992 pisces 0.19 0.44 0.26 1026 sagittarius 0.40 0.23 0.29 993 scorpio 0.64 0.20 0.30 1013 taurus 0.39 0.25 0.31 1051 virgo 0.16 0.47 0.24 1095 accuracy 0.29 12339 macro avg 0.38 0.29 0.30 12339 weighted avg 0.38 0.29 0.30 12339 The new accuracy rate of ~41% is worse than the first iteration, but slightly better than the KNN model. If we check again with cross validation, the new model is still averaging ~8% which is not very good. precision recall f1-score support aquarius 0.08 0.20 0.11 332 aries 0.09 0.18 0.12 316 cancer 0.09 0.13 0.11 390 capricorn 0.05 0.06 0.05 276 gemini 0.08 0.08 0.08 380 leo 0.10 0.07 0.08 393 libra 0.08 0.05 0.06 362 pisces 0.07 0.04 0.05 308 sagittarius 0.09 0.04 0.06 319 scorpio 0.07 0.03 0.05 343 taurus 0.08 0.05 0.06 339 virgo 0.14 0.08 0.10 356 accuracy 0.08 4114 macro avg 0.09 0.08 0.08 4114 weighted avg 0.09 0.08 0.08 411 4.5.6 Final Model So it seems that the knn_model might be the best model for OkCupid to use when users dont have their signs listed on their user profile. By using the hold out or validation set, we get ~8% accuracy which is not very good. precision recall f1-score support aquarius 0.08 0.22 0.12 294 aries 0.09 0.18 0.12 345 cancer 0.09 0.15 0.11 328 capricorn 0.06 0.06 0.06 315 gemini 0.08 0.06 0.07 366 leo 0.10 0.07 0.08 395 libra 0.10 0.06 0.08 326 pisces 0.07 0.05 0.06 337 sagittarius 0.08 0.03 0.05 347 scorpio 0.08 0.04 0.05 355 taurus 0.07 0.04 0.05 345 virgo 0.08 0.05 0.06 361 accuracy 0.08 4114 macro avg 0.08 0.08 0.08 4114 weighted avg 0.08 0.08 0.07 4114 In the confusion matrix, it becomes clear that Cancer, Gemini, Leo, and Virgo was predicted most often, but were not super accurate since the vertical color band represents even distributed guesses mostly wrong and some correct. 4.6 Conclusion In this project, machine learning was used to predict the astrological signs of OkCupid users. This is an interesting feature, as many people believe in astrology and combinations between compatible star signs. If users do not enter their signals, an algorithmic solution may have generated a signal to input the missing data when making matches. Unfortunately, the final algorithm selected was no better than basic guessing, showing that not everything is appropriate for using machine learning as many people say out there. "],["real-estate-são-paulo-rent-prediction.html", "Project 5 Real Estate São Paulo Rent Prediction 5.1 Introduction 5.2 Data 5.3 Model 5.4 Prediction 5.5 Use Case of the Model 5.6 Conclusion", " Project 5 Real Estate São Paulo Rent Prediction Code: GitHub Web App: sp-rent-predictions 5.1 Introduction Everyone needs a place to live. It can be a house, a flat, or an apartment. At some point in life, everyone is faced with the choice of buying or renting a house. On top of that, the following questions can be asked: Is buying a property worthwhile or staying in rent is the best alternative? In a society that values and idealizes the dream of homeownership, those who question themselves about the subject end up being seen as crazy. After all, from an early age, we believe that homeownership is a goal that everyone should have and strive to achieve. This, however, is not an absolute truth. Furthermore, having a home, for many, is synonymous with stability and security. But, is it worth buying a house? Or can continuing to rent make more sense? Lets point out some of the reasons that it makes sense to rent and why this is such an attractive business: Renting is not synonymous with losing money Considering the fixed income, depending on how the interest rates are, for example, it is more worthwhile to continue renting and invest the money that was reserved for the home itself. Thus, it is possible to pay the lease with the proceeds and save the rest too, who knows, buy it later. After all, the vast majority of people do not buy their houses in cash and resort to financing. And this type of business is not advantageous, as the person ends up assuming debt for years, and the amount to be paid for the property can even triple in comparison with the original value of the property. In this way, it can be much more worthwhile to invest your money - making it work for you - and to continue renting, paying this expense (or part of it) with the income. Property is not an investment It is important to keep in mind that owning a home is only for the sake of the family and does not generate any profit. With this new acquisition, debts such as property tax and other taxes, maintenance expenses, and a series of repairs that may appear will come. Certainly, your money will be invested there, but it can both appreciate - with the appreciation of the good itself over the years - as well as devalue itself. Not to mention that the money is immobilized, causing you to lose liquidity. The property is not yours until it is paid off The property is not yours! Yes, the apartment will be yours only after paying off the debt. That is, you will only be the owner after the twenty years that you were paying for the good! Is it worth it to spend all that time paying off a debt? Depending on the contract signed, you will only be the owner after a few decades In addition to the reasons mentioned above, São Paulo is a very busy city and many people go to it to work or study for a while, and renting makes a lot more sense than buying. Regardless of the reasons for renting, the sides need to agree on a price. It is always good to know how much a property is worth, what the expected transaction price is. 5.1.1 Analysis In this work, we want to find an answer to these questions. This case study is intended to be a comprehensive use case of dealing with a regression problem for Data Science. We will start with some questions that allow us to understand the issues related to renting a property. The owner does not know how to increase the propertys value so that the investment is less than the added value. For example, building a swimming pool can increase the price, and forming a bathroom is not worth it. The owner does not know how much he will rent a property. He makes an offer on the portal and doesnt know if the price is right, The tenant does not know how much the property is worth. These are some of the questions that can be asked. As a definition of our problem, we define the propertys valuation, and thorough explanations, we try to get an answer depending on the position we choose. Initially, we will see how the literature says the amount of rent should be charged. According to, the basic rule most used to calculate rent is to apply a value between 0.5% and 1%, per month, on the propertys market value. This means that if the property is worth R$ 100 thousand, the rent must be between R$ 500 and R$ 1,000 per month. Research shows that, on average, two- and three-bedroom houses and apartments, the rental price is 0.75%. But many variables make up the rent calculation. The real estate market itself regulates itself and prices the rental price of properties by supply and demand. Supply and demand are essential concepts of the Liberal Economy. The propertys characteristics are also decisive in the calculation of rent. Similar properties in the same region of the city, in general, have approximate values. That is why it is always important to observe the values in force in the real estate market in the region in which you will invest or prospect your property for business. There are similar properties in the same neighborhood that may have different rentals. This is justified by the commercial and transport structure available. Factors such as the total length of the property, leisure facilities, swimming pool, party room, condominium, concierge services, security, garage, number of rooms influence the calculation of rent An apartment or house, for example, close to bus or metro stations, may have a higher rental price than a more remote one. The same goes for properties in regions with supermarkets, pharmacies, and bakeries located next to a park. With this premise, we can now analyze the data. 5.2 Data As a data source for information on property characteristics and prices, a set of data obtained from a web scraper on the Zap Imóveis website was used. Web Scraper is a tool to extract data from a HyperText Markup Language (HTML) website. The web scraper was developed by GeovRodri based on Beautiful Soup and made available through the link: zapimoveis-scraper The set of data collected on the website is based on rental advertisements in the city of São Paulo containing more than 60 thousand data. The data set obtained contains the data according to TABLE 5.1. TABLE 5.1.1: Description of variables in the dataset. Variable Description description property description price price (monthly) bedrooms number of bedrooms on property bathrooms number of bathrooms on property total_area_m2 property area (square meters) vacancies parking spots available on property address property address link link of the property Another idea on how to enrich our solution was to add external data. Our hypothesis took into account the information given in the introduction of how a property is calculated, therefore, the location of the property can significantly affect the price. Therefore, we also take into account the distance to the nearest public transport (Metro and bus), distance to the nearest school, number of cultural variety within 1km, and addition to the crime rate in the region. 5.2.1 Data Preparation Like all automatic data collection processes, the web scraper described the above-generated data with quality problems. To solve this problem, we apply some cleaning processes. The first filter was to remove instances that had null values (absent) for attributes considered fundamental for the analysis. To eliminate outliers that left the data inconsistent, such as, for example, rent with the sale price of properties, places with more than 50 bathrooms, and houses with areas the size of a city, conditions were added for the property to be considered consistent with reality. 5.2.2 External data External data has been added. We believe that many other variables describe the rental price. Geospatial information was missing. Why do some places tend to be more expensive? Perhaps it is because of the availability of public transport. That is why we decided to add, for example, a variable that describes the distance to the nearest bus or metro station. In the data described, we note that unfortunately there is no zip code and geolocation of the properties. To our happiness, the website provides an address, and from that, it was possible to use a geolocation tool called Geopy available for Python. With it, it is possible to enter an address and automatically the library provides you with the longitude and latitude of the location. fter obtaining the geolocation values of the properties, a second cleaning was performed on the data. Many registrations and inconsistencies of Geopy cause some locations of the respective properties to be outside the city of São Paulo (curiously some even in the middle of the Atlantic Ocean). To solve this problem, we took the official data from the location of the city of São Paulo, made available by IBGE and with the help of Geopandas, we applied the data so that all the properties in our database were within the region. After another treatment of data, we added variables that describe the distances to the bus, subway station, and school closest to the property. The data were obtained from the GeoSampa website provided by the city of São Paulo. Another reason we took into account the price of renting the property was that they may be more expensive because of some interesting places around, such as museums, galleries, shopping centers, or libraries. These places are not just stand-alone facilities but are connected to other infrastructure, which is generally expected to increase the price. For this variable, we consider a radius of 700 meters chosen arbitrarily. The data were again obtained from the GeoSampa website. With the same situation in mind, we also added a variable called food to calculate the number of places to eat within a radius of 700 meters. We consider restaurants, bars, cafes, among others. To obtain the data, we use the Overpass Turbo site which is a web-based data mining tool for OpenStreetMap. Finally, we also decided to add a crime rate variable around real estate, as we believe that more violent places tend to have lower rent values. The data were obtained through the Geospatial São Paulo Crime Database made available by Kaggle. The data can be viewed in FIGURE X. There are 21367 bus stops, 89 metro stations, 4761 schools, 1697 cultural points and 12899 crime data. TABLE 5.1.2: Description of the external variables in the dataset. Variable Description Id property Id Type house / apartment Region region of the city dist_subway distance from properly to the nearest subway station dist_bus distance from properly to the nearest bus station school distance from properly to the nearest school ncult number of cult place in area of the property food number of food place in area of the property crime% % of crime in property area latitude latitude of the property longitude longitude of the property items Items of the property FIGURE 5.1: Spatial external data. Stops Bus on the left and Subway Station on the right. Each point indicates the location of the property (marked in red). The blue and indicate the location of public transport stops. On the x-axis we have longitude values, while on the y-axis we have latitude values. FIGURE 5.2: Spatial external data. Schools on the left and cultural places on the right. FIGURE 5.3: Food Place on the left and heatmap of crime rate on the right Most cultural locations are located in the city center. Thus, this variable also tells the story of how much city center the property has. As all these places are in the center of the city, these other points must reflect some local centers. As can be seen in the figure, bus stops and schools are very dense across the city. Showing that all properties have a good connection to public transport and education. This can, for example, facilitate the movement of children to schools and parents to workplaces. But the problem of transport via the Metro is evident on the map. We see that it covers a very small region of the city, being that it is the fastest means of transport, which probably properties in its surroundings must have rents much more expensive than the rest of the region. In the last figure, we plot the density of crime in the city. We see that the center has a much higher density than the rest of the region. After doing this localization step, an id was created for the locations, noting that the property link exists a unique id for each one of them. This was important to eliminate duplicate data. The last step was to divide the data into training and test samples at random with a 70/30 ratio to apply machine learning. The script to download, clean, and process the data can be found on GitHub. For the analysis of spatial data, a small geofast script was made. The distances between 2 arbitrary points on the earth can be obtained in the Geopy package for Python. However, the general equation is very computationally expensive for this work. Then an adjustment was made in our case without losing almost any precision. A simple equation of geospatial calculation was used that can be applied for short distances of a few hundred kilometers. 5.2.3 Text Processing To extract information from the title and description, we followed the regular Natural Language Processing pipeline, which encompasses tokenization (splitting sentences into separate words), removal of common unhelpful words such as prepositions (stopwords removal), and lemmatization (reduction of words to their root forms). From the texts, we add variables that we think are important for the price of the property. For example, if it is a townhouse, if it has a swimming pool, barbecue, if it is furnished, and so on. 5.3 Model In this project, We will be using the linear model as a base model to finalize the data pre-processing steps. We will use outlier removal techniques, coding techniques, sizing/normalization techniques, resource engineering, and different types of imputation combinations. We divided the modeling into five steps. Step1: Linear Models In this step We used linear models like Linear Regression, Bayesian Ridge Regression, Lasso, Elastic Net and Ridge. Step2: Support Vector Machines We used SVR(Support Vector Regressor). Similar to linear models for SVR also. Step3: Ensemble Methods In first two steps We used basic models, from step 3 onward models becomes complex. We used models like Gradient Boosting Regressor, LightGBM Regressor, XGB Regressor. Step4: Model Stacking Stacking is an ensemble learning technique to combine multiple regression models via a meta-regressor. At this stage we have good mix of multiple models, now is the best time to use StackingCVRegressor to combine all above models to improve the score even further. As expected this model resulted in best possible score! Step5: Model Blending Model blending is the manual step, where we manually adjust the weight for each model in order to ensemble the predictions to improve the score. Thumb rle is to give highest weights to best performing model and lowest to least performing model. After few trial and and error I could settle on the weights and manage to top my model stacking score! 5.3.1 Root Mean Square Error (RMSE) Root Mean square is the standard deviation of the residuals. Now lets understand what Standard deviation and residuals are. Standard deviation: Standard deviation measures how to spread out numbers are. Its formula is the square root of the Variance. Variance is defined as the average of the squared differences from the mean. In the below formula of standard deviation xi= numbers, = Mean of the numbers, and N = Total number of values. Residuals: Residuals measure how far from the regression line data points are. Residuals are nothing but prediction errors. We can find it by subtracting the predicted value from the actual value. To get RMSE, we will use the Standard deviation formula, but instead of the square root of Variance, we will calculate the square root of the average of squared residuals. Standard deviation is used to measure the spread of data around the mean, while RMSE measures the distance between predicted and actual values. RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to significant errors. This means that the RMSE is most useful when significant errors are particularly undesirable. As per competition rules, submissions are evaluated on the logarithm of the predicted value and the logarithm of the observed sales price. Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally. What does RMSE indicate?: It indicates the absolute fit of the model to the data. Provides average model prediction error in units of the variable of interest. They are negatively-oriented scores, which means lower values are better. 5.3.2 Correlation Check Correlation is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other. The logic behind using correlation for feature selection is that the suitable variables are highly correlated with the target. A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. We will use Pearsons correlation. It is utilized we you have two quantitative variables, and you wish to see if there is a linear relationship between those variables. FIGURE 5.4: Correlation Map of the variables. In above correlation map: -1 indicates a perfectly negative linear correlation between two variables. 0 indicates no linear correlation between two variables. 1 indicates a perfectly positive linear correlation between two variables. 5.3.3 Multicollinearity Analysis Multicollinearity refers to when more than two explanatory variables in a multiple regression model are highly linearly related. Below are a few of the multicollinear features based on the correlation matrix. FIGURE 5.5: Correlation of Price vs dist subway, dist bus, dist school and live area. FIGURE 5.5: Boxplot of correlation of Price vs numbers Bathrooms, Bedrooms and Vacancies 5.3.4 Feature Engineering 5.3.4.1 Numeric Feature Scaling To give every feature the same importance, we perform feature scaling. There are many techniques like Min-Max Scaler, Robust Scaler, etc., to do feature scaling. Before we can finalize any scaling technique, lets check the skewness of our numeric features. Skewness is the measure of the degree of asymmetry of a distribution: skewness = 0 : normally distributed. skewness &gt; 0 : more weight in the left tail of the distribution. skewness &lt; 0 : more weight in the right tail of the distribution. 5.3.4.2 Target Variable Analysis and Transformation Price is our target variable. If we want to predict the target variables accurately, then the first task is to understand the underlying behavior of our target variable. The model can make more reliable predictions if our target variable is normally distributed. We will use both graphical and statistical methods to test our target variables normality. Statistical We will use Skewness, Kurtosis and Shapiro-Wilk test for normality. Skewness assesses the extent to which a variables distribution is symmetrical. The thumb rule is, If the skewness is between -0.5 and 0.5, the data are fairly symmetrical. If the skewness is between -1 and  0.5 or between 0.5 and 1, the data are moderately skewed. If the skewness is less than -1 or greater than 1, the data are highly skewed. Kurtosis tells you the height and sharpness of the central peak, relative to that of a standard bell curve. The thumb rule is, Kurtosis greater than +1 indicates distribution is too peaked. Kurtosis less than 1 indicates a distribution that is too flat. FIGURE 5.6: Rent Price Normal Distribution From a standard distribution plot, its clear that the mean is more significant than mode similarly from the probability plot, we can see that most of the observations fall on the lower end of the Y-axis. So we can conclude that the target variable Price is right-skewed. There are multiple transformation techniques to handle the skewed data. We use log transformation. FIGURE 5.6: Rent Price Normal Distribution after apply Log 5.4 Prediction 5.4.1 Linear Models 5.4.1.1 Linear Regression Linear Regression fits a linear model with coefficients w = (w1, , wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. TABLE 5.2: Results for Random Forest with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3829 ± 0.0050 0.3798 5.4.1.2 Bayesian Ridge Regression Bayesian Ridge estimates a probabilistic model of the regression. Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand. The advantages of Bayesian Regression are: It adapts to the data at hand. It can be used to include regularization parameters in the estimation procedure. The disadvantages of Bayesian regression include: Inference of the model can be time consuming. TABLE 5.3: Results for Random Forest with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3829 ± 0.0050 0.3797 5.4.1.3 Lasso Linear Model trained with L1 prior as regularizer. TABLE 5.4: Results for Lasso with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3831 ± 0.0050 0.3801 5.4.1.4 Elastic Net Linear regression with combined L1 and L2 priors as regularizer. TABLE 5.5: Results for Elastic Net with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.4064 ± 0.0031 0.4035 5.4.1.5 Ridge Linear least squares with l2 regularization. This model solves a regression model where the loss function is the linear least-squares function, and regularization is given by the l2-normalso known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets)). TABLE 5.6: Results for Ridge with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.4064 ± 0.0031 0.4035 5.4.1.6 Support Vector Machines Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. The model produced by Support Vector Regression depends only on a subset of the training data because the cost function ignores samples whose prediction is close to their target. TABLE 5.7: Results for Support Vector Machines with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3461 ± 0.0064 0.3427 5.4.2 Ensemble Methods Ensemble methods aim to combine the predictions of several base estimators built with a given learning algorithm to improve generalizability/robustness over a single estimator. Two families of ensemble methods are usually distinguished: In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any single base estimator because its variance is reduced. Examples: Bagging methods, Forests of randomized trees, etc By contrast, in boosting methods, base estimators are built sequentially, and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble. Examples: AdaBoost, Gradient Tree Boosting, etc 5.4.2.1 Gradient Boosting Regressor GBR supports several different loss functions for regression which can be specified via the argument loss; the default loss function for regression is least squares (ls). TABLE 5.8: Results for Gradient Boosting with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3148 ± 0.0050 0.3128 5.4.2.2 LightGBM Regressor (Light Gradient Boosting Machine) LightGBM is a gradient boosting framework based on decision trees to increase the models efficiency and reduce memory usage. It uses two novel techniques: Gradient-based One Side Sampling and Exclusive Feature Bundling (EFB), which fulfills the histogram-based algorithms limitations that are primarily used in all GBDT (Gradient Boosting Decision Tree) frameworks. TABLE 5.9: Results for LightGBM with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3152 ± 0.0038 0.3198 5.4.2.3 XGB Regressor XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. XGBoost stands for Extreme Gradient Boosting We can use it to solve classification and regression problems. The XGBoost is a popular supervised machine learning model with computation speed, parallelization, and performance characteristics. TABLE 5.10: Results for XGBoost with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3164 ± 0.0040 0.3124 5.4.3 Model Stacking 5.4.3.1 Stacking CV Regressor An ensemble-learning meta-regressor for stacking regression. Stacking is an ensemble learning technique to combine multiple regression models via a meta-regression. The StackingCVRegressor extends the standard stacking algorithm (implemented as StackingRegressor) using out-of-fold predictions to prepare the input data for the level-2 regressor. In the standard stacking procedure, the first-level regressors are fit to the same training set used to prepare the inputs for the second-level regressor, which may lead to overfitting. The StackingCVRegressor, however, uses the concept of out-of-fold predictions: the dataset is split into k folds, and in k successive rounds, k-1 folds are used to fit the first level regressor. In each round, the first-level regressors are then applied to the remaining subset that was not used for model fitting in each iteration. The resulting predictions are then stacked and provided  as input data  to the second-level regressor. After the training of the StackingCVRegressor, the first-level regressors are fit to the entire dataset for optimal predictions. Ref. &lt;http://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/&gt; FIGURE 5.7: Stacking Model TABLE 5.11: Results for Stacking CV Regressor with 5-Fold Cross-Validation. RMSLE on Training (CV = 5) RMSLE on Test 0.3112 ± 0.0048 0.3198 5.4.3.2 Model Blending And finally We use Model Blending to combine all models together. TABLE 5.12: Results for Model Blending with 5-Fold Cross-Validation. RMSLE on Training RMSLE on Test 0.2973 0.2923 5.4.4 Summary TABLE 5.13: Summary all models Model RMSLE on Training RMSLE on Test R2 on Teste Linear Regression 0.3829 ± 0.0051 0.3798 Bayesian Ridge 0.4134 ± 0.0031 0.4145 Lasso 0.3831 ± 0.0050 0.3801 Elastic Net 0.4064 ± 0.0031 0.4035 Ridge 0.3829 ± 0.0050 0.3798 Support Vector Machine 0.3461 ± 0.0064 0.3427 Gradient Boosting 0.3148 ± 0.0050 0.3128 LightGBM 0.3190 ± 0.0026 0.3180 XGBoot 0.3164 ± 0.0040 0.3124 Stakcing CV 0.3112 ± 0.0048 0.3198 Blending 0.2973 0.2923 5.5 Use Case of the Model Having the model trained, tested, and explained, it is time for the use cases. The first application can be someone who wants to rent a house and looks for more attractive values. With the model, it is possible to search for regions of interest and find out if the rental amount that the owner is charging is worth it or not. In addition, it is possible to search for properties by price range. The second application is for the owner who wanted to rent his property. Knowing the regions value, he can make his rent much more competitive against the competition of people who are also selected for renting. In addition, he can estimate with variables what is possible to add to his property or as options in the region to increase the value. 5.6 Conclusion Predicting real estate rent prices from online advertisements in Brazil is a task that requires insight into the data combined with powerful ML algorithms. Many locations in the city of São Paulo do not have standards in the choice of price, which makes it very difficult for machine learning models. In this work, we applied nine different methods for this task and combined them into a final prediction. We provided a solid baseline to overcome, as our final ensemble hit a score of 0.2973 RMSLE showing how powerful the mixed model methods are. For future work, we can make a web scraper again on the site, take images of the properties and apply a convolutional neural network, aggression, that could help the models accuracy with real photos inside the properties. "],["hotmart-sales-analytics.html", "Project 6 Hotmart Sales Analytics 6.1 Introdução 6.2 Dados 6.3 Exploratory Data Analysis (EDA) 6.4 Análise de Séries Temporais", " Project 6 Hotmart Sales Analytics Code: GitHub 6.1 Introdução Esse trabalho é parte de um desafio proposto pela empresa Hotmart. Hotmart é uma empresa global de tecnologia e educação, com uma plataforma online para distribuição e venda de produtos digitais. A análise deste trabalho é baseada em uma amostra de compras feita na Hotmart no ano de 2016. Tratam-se de 1,5 milhão de registros de compras realizados na plataforma. 6.2 Dados O primeiro passo em qualquer análise é entender os dados. Vamos dar uma olhada no dataset disponibilizado. Os dados são disponibilizados pela Hotmart e se trata de uma amostra de compras feita pela Hotmart em 2016. Vamos começar carregando os dados e conferindo cada um deles e seu tipo. Podemos ver que o dataset tem 1599829 amostras e 14 colunas. De acordo com a Hotmart, cada campo significa: Variáveis Descrição purchase_id Identificação da compra na Hotmart product_id Identificação do produto na Hotmart; affiliate_id Identificação do afiliado na Hotmart; producer_id Identificação do produtor na Hotmart; buyer_id Identificação do comprador na Hotmart; purchase_date Data e hora em que a compra foi realizada; product_creation_date Data e hora em que o produto foi criado na Hotmart; product_category categoria do produto na Hotmart. Exemplo: e-book, software, curso online, e-tickets, etc.; product_niche nicho de mercado que o produto faz parte. Exemplo: educação, saúde e bem-estar, sexualidade, etc.; purchase_value valor da compra. Esse dado, assim como nicho e categoria foi codificado para manter a confidencialidade. O valor apresentado no dataset é o z-score do valor real; affiliate_commission_percentual percentual de comissão que o afiliado receberá da compra purchase_device: tipo de dispositivo utilizado no momento da compra, como: Desktop, Mobile, Tablet, ou Outros; purchase_origin endereço do site do qual a pessoa veio antes da compra. Por exemplo, se uma pessoa veio do Facebook, Youtube, ou até mesmo de outra página no site oficial do produto is_origin_page_social_network informa se essa compra veio de uma URL do Facebook, Youtube, Instagram, Pinterest, ou Twitter. 6.3 Exploratory Data Analysis (EDA) Vamos olhadar agora as primeiras 5 linhas do dataset: Podemos ver que na primeira linha existem valores NaN, o que é um problema para as análises que serão feitas. Vamos então conferir quantas linhas do dataset existem esse com esse tipo de dados. purchase_id 0 product_id 1 affiliate_id 1 producer_id 1 buyer_id 1 purchase_date 1 product_creation_date 1 product_category 1 product_niche 1 purchase_value 1 affiliate_commission_percentual 1 purchase_device 1 purchase_origin 1 is_origin_page_social_network 0 Podemos ver que existe somente uma linha de dados com dados do tipo NaN, que como já vimos, trata-se da primeira linha do dados. Vamos então remove-los. 6.3.1 Variáveis Contínuas 6.3.1.1 purchase_value Agora podemos dar uma olhada geral nos dados numéricos. Da tabela abaixo vemos que a variável purchase_value tem valores negativos, o que não faz sentido para um valor de compra. No entanto, pela descrição, sabemos que ela foi codificada em termos de z score, que nada mais é do que o número de desvios padrões em relação a média de um ponto de informação. Olhando graficamente existe uma variação entre -0.5 e 1. Podemos utilizar a técnica de normalização MinMaxScaler e assim transformar o intervalo entre 0 e 1 e dessa forma teremos percentualmente o faturamento. 6.3.2 Variáveis Descretas Nessa seção, iremos dar uma olhada nas variáveis discretas para procurar por tendências. 6.3.2.1 product_category Analisando o gráfico de categoria de produtos, vemos que a maior concentração de vendas na plataforma Hotmart no ano de 2016 foi em livros físicos, representando quase 80% dos casos. Seguidos por Podcast perto dos 20% e Workshop quase todo o resto. 6.3.2.2 product_niche Analisando o gráfico de nichos, vemos que as maiores concentrações de vendas na plataforma Hotmart no ano de 2016 estavam em na categoria de negociação, gerenciamento de ansiedade (autocontrole), que juntos representam quase 30% dos casos. Seguidos por finança pessoal e habilidade de apresentações que somados representam quase 20%. 6.3.2.3 purchase_device Analisando o gráfico de dispositivos onde são feitas as compras, vemos que a maior concentração de vendas na plataforma Hotmart no ano de 2016 foi em eReaders, representando 40% dos casos. Seguidos por Desktop na casa dos 35% e Smart TV com 20%. 6.3.2.4 is_origin_page_social_network Por último, o gráfico se a compra tem origem de rede sociais, vemos que quase a totalidade está com 0. Sobre esta categoria não ficou claro se 0 representa Sim ou Não 6.3.3 Segmentação De Vendas Por Produto Agora que conhecemos as variáveis discretas e contínuas, vamos analisar as vendas segmentadas por produto. Da tabela gerada, vemos que os produtos mais vendidos foram livros físicos, como era de se esperar, visto que representam quase 80% de vendas na plataforma, focado no nicho de imigração, criação de conteúdo por Youtubers, e finanças. A maioria das compras vieram de eReaders e Smart TV. Na tabela acima é possível ver que no quesito de faturamento, o top 10 são todos livros físicos focados no mesmo nicho. O curioso é que as compras foram feitas na maior parte nos eReaders, o que fica o questionamento se a categoria de produtos está realmente correta. Por último, temos que 4 dos 10 produtos mais vendidos na plataforma também são os produtos que mais geraram receitas. Agora vamos analisar a relevância de vendas dos produtos na Hotmart. top 1 Sales 2.58 % top 10 Sales 14.31 % top 50 Sales 28.36 % top 100 Sales 37.28 % top 500 Sales 61.94 % top 1000 Sales 73.54 % top 1 Revenue 1.53 % top 10 Revenue 9.93 % top 50 Revenue 27.66 % top 100 Revenue 38.51 % top 500 Revenue 67.08 % top 1000 Revenue 79.07 % Dos resultados, podemos ver que os 10 produtos que mais deram lucros para a Hormat no ano de 2016 também foram responsáveis por quase 10% de todo o lucro da empresa. Vemos também que 50% dos produtos mais vendidos, são responsáveis por quase 30% de todo o faturamento. No quesito de quantidade de vendas, vemos que o padrão é muito parecido com a quantidade de lucro da empresa. 6.3.4 Segmentação Das Vendas Por Produtor de Conteúdo Iremos fazer a mesma análise de seção anterior, mas focado no produtor de conteúdo. Temos que 2 dos 10 produtores que mais vebdem na plataforma também são os produtos que mais geraram receitas. Agora vamos analisar a relevância de vendas dos produtores de conteúdo na Hotmart. top 1 Sales 2.58 % top 10 Sales 17.61 % top 50 Sales 36.58 % top 100 Sales 48.55 % top 500 Sales 79.79 % top 1000 Sales 89.94 % top 1 Revenue 2.18 % top 10 Revenue 15.77 % top 50 Revenue 42.69 % top 100 Revenue 57.56 % top 500 Revenue 87.11 % top 1000 Revenue 94.43 % Dos resultados, podemos ver que os 10 dos produtores de conteúdo que mais deram lucros para a Hormat no ano de 2016 foram responsáveis por quase 15% de todo o lucro da empresa. Vemos também que 100 dos produtores de conteúdo que mais deram lucros, são responsáveis por quase 50% de todo o faturamento daquele ano. No quesito de quantidade de vendas, vemos que o padrão é muito parecido com a quantidade de lucro da empresa. Podemos concluir que o Hotmart depende dos maiores produtores da plataforma. 6.4 Análise de Séries Temporais Uma série temporal é uma sequência de pontos de dados obtidos em pontos sucessivos e igualmente espaçados no tempo que podem ser usados para prever o futuro. Um modelo de análise de séries temporais envolve o uso de dados históricos para prever o futuro. Ele procura no conjunto de dados recursos como tendências, flutuações cíclicas, sazonalidade e padrões comportamentais. 6.4.1 Há épocas do ano em que uma determinada categoria ou nicho de produto vende mais? Do gráfico, podemos ver que o mês que tiveram venda na plataforma foi no mês de maio. 6.4.2 A que horas devemos exibir anúncios para maximizar a probabilidade de compra do produto pelo cliente? Como você pode ver na Figura, existem aproximadamente 2 picos nos dados. São das 10h às 11h e um menor às 21h . Faz sentido, já que a maioria das pessoas faz compras durante o dia. A partir desses dados, podemos sugerir ao nosso parceiro comercial que anuncie seu produto antes das 12h e/ou 19h. Pode ser 9h30 e/ou 20h30. 6.4.3 Análise de Predições As três principais ideias gerais que são fundamentais a serem consideradas para a predições de séries temporais, ao lidar com um problema de previsão de vendas abordado a partir de uma perspectiva de série temporal, são: Padrões de repetição Padrões estáticos Tendências 6.4.3.1 Padrões de Repetição Vamos começar analisando a série temporal por quantidade de vendas e lucros e procurar se existem padrões de repetição. Da tabela e do gráfico podemos ver que o dia que mais teve movimentação foi no dia 2 de maio com quase o dobro das próximas datas do top 10. Olhando somente os dados fornecidos não há muito de especial para explicar esse fato. Olhando o top 10 de maiores transações na plataforma, notamos que outras datas em maio também tiveram grande volume de vendas. Analisando o gráfico de autocorrelação, vemos alguns pontos de correlação relativamente alta em lag 2 e 7. A falta de uma estrutura bem definida é resultado das contingências de vendas: dado o número de fatos que entram na previsão de vendas, não devemos esperar que os dados tenham correlações perfeitamente claras como em um conjunto de dados meteorológicos, por exemplo. No entanto, é interessante observar picos de correlação que podem estar associados a fatores relacionados ao tipo de produto envolvido. Já em relação ao lucro, podemos ver da tabela e gráfico que os maiores lucros ocorreram no mês de março e junho. Podemos ver também que o gráfico de autocorrelação tem um padrão muito parecido com o de quantidade de vendas. Resultados de vendas: ADF = -3.0875884164950738 p-value = 0.02748544055310883 Resultados de Lucros: ADF = -3.6274761082029476 p-value = 0.005256955239645174 Podemos ver dos testes que o valor p indica uma confiança não significativa de que os dados seriam estacionários 6.4.3.2 Média móvel Este modelo assume que a próxima observação é a média de todas as observações anteriores e pode ser usado para identificar tendências interessantes nos dados. Podemos definir uma janela para aplicar o modelo de média móvel para suavizar a série temporal e destacar diferentes tendências. Para o conjunto de dados de vendas, o ajuste não parece tão promissor. Além disso, o parâmetro de janela que define o tamanho de nossa média tem um grande efeito em nosso desempenho geral e não foi feito nenhum ajuste de hiperparâmetro adicional. Aqui, o que devemos levar em consideração é que conjuntos de dados de vendas complexos exigirão mais informações do que uma simples série temporal unidimensional pode fornecer. 6.4.3.3 Arima ARIMA ou Auto-regressive Integrated Moving Average é um modelo de séries temporais que visa descrever as autocorrelações nos dados de séries temporais. Funciona bem para previsões de curto prazo e pode ser útil para fornecer valores previstos para períodos especificados pelo usuário, mostrando bons resultados para demanda, vendas, planejamento e produção. Os parâmetros do modelo ARIMA são definidos da seguinte forma: p: O número de observações de atraso incluídas no modelo d: O número de vezes que as observações brutas são diferenciadas q: O tamanho da janela de média móvel MSE error: 7473419.795985948 Podemos ver que tivemos um resultado muito ruim com o MSE bem alto. Vamos apenas lembrar que os resultados aqui são apenas para mostrar os modelos e não representam uma estimativa precisa. Os conjuntos de dados são limitados (o conjunto de dados de vendas após a soma é menor que 200 pontos de dados) e não foi realizado nenhum ajuste de hiperparâmetro complexo. O objetivo aqui foi apenas demonstrar como esses modelos funcionam e como eles podem ser implementados. Podemos verificar que o conjunto de dados de vendas parece apresentar desafios que os modelos tradicionais não conseguem superar. Podemos ver que, para conjuntos de dados que possuem um padrão claro, os modelos tradicionais funcionam bem. No entanto, na ausência de tal estrutura, esses modelos não parecem apresentar flexibilidade para se adaptar, pois dependem de fortes suposições sobre a dinâmica da série temporal alvo. "],["telecom-clusters-analysis.html", "Project 7 Telecom Clusters Analysis 7.1 Análise Exploratória dos Dados 7.2 Análise de Clusters 7.3 Clusters não-Hierárquicos 7.4 Interpretação e Validação do Modelo 7.5 Conclusão", " Project 7 Telecom Clusters Analysis 7.1 Análise Exploratória dos Dados A clusterização pode ser usada para agrupar clientes com base em semelhanças. A segmentação de clientes pode ser usada para criar estratégia de marketing apropriada para o segmento. Nesta atividade iremos analisar a clusterização k-means utilizando o dataset de uma operadora de Telecom. O dataset é composto por 1500 clientes e iremos analisar as variáveis de tempo médio usado em chamadas de longa distância, local e internacional para fazer a clusterização. Vamos iniciar explorando os dados: ## longdist internat local ## Min. : 0.00 Min. : 0.00 Min. : 8.19 ## 1st Qu.:20.17 1st Qu.: 0.00 1st Qu.: 49.57 ## Median :25.98 Median : 1.84 Median : 59.83 ## Mean :27.40 Mean : 5.45 Mean : 65.61 ## 3rd Qu.:35.66 3rd Qu.:10.16 3rd Qu.: 70.08 ## Max. :62.47 Max. :29.02 Max. :134.76 Dos dados, vemos que as 3 variáveis apresem outliers sendo que a variável local é a que mais tem outliers. Além disso, o valor mínimo de longdist e internat é 0, o que faz sentido já que estamos analisando o tempo médio e variação dos dados é grande. Vamos padronizar e olhar os histograma das variáveis ## Zlongdist.V1 Zinternat.V1 Zlocal.V1 ## Min. :-1.7031825 Min. :-0.801535 Min. :-1.6854918 ## 1st Qu.:-0.4495639 1st Qu.:-0.801535 1st Qu.:-0.4708472 ## Median :-0.0884570 Median :-0.530931 Median :-0.1696811 ## Mean : 0.0000000 Mean : 0.000000 Mean : 0.0000000 ## 3rd Qu.: 0.5131805 3rd Qu.: 0.692667 3rd Qu.: 0.1311915 ## Max. : 2.1794926 Max. : 3.466352 Max. : 2.0297709 Do histograma podemos ver que a variável longist é que mais se aproxima de uma distribuição normal, já a variável internat temos uma distribuição skew à esquerda. 7.1.1 Estatística de Hopkins Uma primeira análise que podemos começar é avaliar se existe estrutura nos dados para a formação de clusters, pois sabemos que sempre existirá formação de clusters nos dados. Para fazer essa análise podemos utilizar a estatística de Hopkins (H) que tenta analisar se de fato existe uma estrutura de clusters nos dados. O que o algoritmo faz é gerar pontos aleatórios fora da amostra e selecionar pontos dentro da amostra e calcular a distância entre os pontos que foram pegos na amostra até o ponto mais próximo. Se o valor \\(\\mathbf{H}\\) for maior que 0.75, então existe uma tendência de clusterizações nos dados com 90% de confiança. # Análise Preliminar dos Dados hopkins(exemplo7[1:3], m=nrow(exemplo7)*0.15) ## [1] 0.9991904 Do resultado nos nossos dados, obtemos um valor \\(\\mathbf{H} = 0.99\\), o que mostra que o nosso dataset tem sim dados clusterizados. 7.1.2 Heatmap Vamos ver o heatmap do dendrograma do dataset. Um dendrograma é um diagrama de árvores que agrupa as entidades que estão mais próximas umas das outras. Conforme mostrado acima, o dendrograma reordena as observações com base na proximidade entre os dados usando alguma métrica de distância (neste caso a euclidiana). A árvore à esquerda do dendrograma representa a distância relativa entre nós. Podemos ver do dendrograma internat se juntando com longdist para depois se juntarem com local. No gráfico hierárquico na esquerda não está muito claro quantos grupos podemos ter, mas aparentemente desta análise inicial parece convergir para 3 grupos. Vamos agora dar uma olhada na Matriz de Dissimilaridade Da Matriz de Dissimilaridade é possível ver um grande grupo seguidos de um grupo médio e de 2 grupos menores. 7.1.3 Visualização 3d Da visualização 3d podemos ver novamente 3 grupos assim como quando fizemos a análise do heatmap e um quarto grupo no eixo da variável local. Sabemos dos gráficos de boxplot que esses dados são possivelmente outlier, além disso, do gráfico de dissimilaridade vimos um pequeno grupo no quanto superior direito do gráfico que pode ser explicado por esses dados. 7.1.4 Análise de Multicolinearidade Vamos ver agora como esta a multicolinearidade ## Zlongdist Zinternat Zlocal ## 5.941898 2.049105 5.222342 Do resultado, podemos ver que as variáveis não são redundantes, tendo assim uma colinearidade baixa. Podemos então utilizar as 3 variáveis com segurança para fazer os clusters. 7.2 Análise de Clusters Feito uma exploração inicial nos dados podemos agora de fato trabalhar na análise de Clusters. Definição da Medida de Similaridade: Como as variáveis tem métricas em escalas diferentes, será utilizado a distância euclidiana com padronização. Pressupostos: A amostra é considerada como representativa dos grupos de clientes da empresa Não existe problema de colinearidade Para a análise de clusters será utilizado uma combinações entre os métodos hierárquicos e não-hierárquicos. O método hierárquico será utilizado para especificar os centroides e o não-hierárquico (k-means) será utilizado para realizar a clusterizaçao. Vamos começar a análise dos clusters comparando 3 alternativas (Ward, Average, Centroids) pois são menos sensíveis a outliers. Dos dendrogramas gerados, o Ward aparenta ter o melhor comportamentos para os nossos dados apresentando clusters bem definidos. O método Average e Centroids acabaram gerando muita confusão na clusterização. 7.2.1 Definição da Quantidade de Clusters Próxima etapa é olhar a quantidade ideal de clusters utilizando o método do cotovelo. Do gráfico acima acima, vemos que o ideal número de clusters está entre 2 e 4, que é exatamente o número de clusters que as análises inicias indicavam. Vamos então analisar com essa quantidade de clusters. As 3 soluções parecem ser boas para utilizar na clusterizações de clientes. Por isso vamos fazer o perfil e interpretação para todas elas. 7.2.2 Perfil e Interpretação da Solução de 2 clusters Analisando os grupos pelo bloxplot, vemos que no cluster1 a mediana das 3 variáveis são bem próximas e está entre 1 e 2. No cluster2 a mediana entre longdist e local continuam parecidas com mediana centrado em 0, já a internat fica menor. ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## hcw_2 1 938.8 938.8 2578 &lt;2e-16 *** ## Residuals 1475 537.2 0.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## hcw_2 1 907.7 907.7 2356 &lt;2e-16 *** ## Residuals 1475 568.3 0.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## hcw_2 1 1164.6 1164.6 5517 &lt;2e-16 *** ## Residuals 1475 311.4 0.2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Todos os grupos tem uma diferença estatística. Quando dividimos em dois grupos vemos que são dois grupos distintos sem que exista mistura entre eles. No grupo da esquerda observamos uma agrupamentos de dados do lado esquerdo. Iremos ver mais adiante o que acontece quando aumentarmos o número de clusters. Da análise das componentes principais, utilizando 2 grupos podemos ver o grupo rosa parece ser segmentando em 3 grupos, indicando que talvez uma solução com 4 clusters seja mais indicado. 7.2.3 Perfil e Interpretação da Solução de 3 clusters Analisando os grupos pelo bloxplot, vemos que no cluster1 a mediana das 3 variáveis são bem próximas e está entre 1 e 2. No cluster2 a mediana entre chamadas de longa distância e local continuam parecidas com mediana centrado em 0, já a chamada internacional fica menor igual vimos na análise com 2 clusters. Para o clusters 3, somente chamadas locais tem valores, o que era esperado pelo que comentamos no início e podemos existir grupo de pessoas que somente fazem chamadas locais. 7.2.4 Perfil e Interpretação da Solução de 4 clusters Na análise com 4 clusters vemos uma padrão bem parecido com o que ja vimos com 3 clusters no cluster 4 (lá era cluster 3) e cluster 1. Para o cluster 2 e cluster 3 temos uma mudança com a separação do grande grupo que tínhamos antes. Agora é possível ver que o grupo 2 prioriza mais chamadas locais e de longa distância, já o grupo 4 prefere chamadas internacionais. No clusplot podemos ver que um grupo superpos outro quando aumentando para 4 clusters. Vamos dar uma olhada nas componentes principais. Da solução com 4 clusters, graficamente quando olhamos as componentes principais vemos uma mistura no maior grupo, acontecendo uma superposição entre o clusters azul claro e rosa. Vamos analisar se utilizando os clusters não-hierárquicos nós não conseguimos fazer uma separação melhor. 7.3 Clusters não-Hierárquicos Agora que temos um análise mais clara dos clusters, iremos utilizar a solução para 3 ou 4 clusters para a clusterização não-hierárquica. Vamos iniciar criando os centroides para as duas soluções ## hcw_3 V1 V2 V3 ## 1 1 1.4252845 1.4015138 1.5874899 ## 2 2 -0.1557206 -0.3547145 -0.3002139 ## 3 3 -1.7031825 -0.8015346 -1.3478718 ## hcw_4 V1 V2 V3 ## 1 1 1.4252845 1.4015138 1.5874899 ## 2 2 -0.1696400 -0.6759335 -0.1690270 ## 3 3 -0.1183404 0.5079141 -0.6525141 ## 4 4 -1.7031825 -0.8015346 -1.3478718 Do resultados, obtivemos 2 clusters que têm centroids que fazem intersecçao entre os grupos o que não parece ser melhor que os grupos formados somente pela clusterização hierárquica. Vamos ver agora a formação para 4 clusters. Para 4 clusters temos uma solução muito mais homogenia, com uma melhor separação entre os clusters meio. Quando olhamos o gráfico em 3d com as componentes principais podemos ver uma separação muito melhor do que ocorreu com o caso de clusterização hierárquica. 7.4 Interpretação e Validação do Modelo ## INDICES: 1 ## Zlongdist Zinternat Zlocal ## 1.420743 1.406559 1.579906 ## --------------------------------------------------------------------------------------------- ## INDICES: 2 ## Zlongdist Zinternat Zlocal ## -0.1556963 -0.3585888 -0.2993492 ## --------------------------------------------------------------------------------------------- ## INDICES: 3 ## Zlongdist Zinternat Zlocal ## -1.7031825 -0.8015346 -1.3478718 ## INDICES: 1 ## Zlongdist Zinternat Zlocal ## 1.425285 1.401514 1.587490 ## --------------------------------------------------------------------------------------------- ## INDICES: 2 ## Zlongdist Zinternat Zlocal ## -0.1608099 -0.6250803 -0.1811889 ## --------------------------------------------------------------------------------------------- ## INDICES: 3 ## Zlongdist Zinternat Zlocal ## -0.1312782 0.7149430 -0.7684430 ## --------------------------------------------------------------------------------------------- ## INDICES: 4 ## Zlongdist Zinternat Zlocal ## -1.6995970 -0.7981076 -1.3458312 7.4.1 Idade Vamos analisar o perfil demográfico começando pela idade. Com 3 grupos temos um perfil bem semelhantes entre os grupos. A mediana de todos eles estão entre 40 e 50 anos. Quando analisamos 4 grupos a mediana também apresenta uma idade entre 40 e 50 anos. Com uma mediana menor para o grupo 4 que só fazem ligações locais. 7.4.2 Sexo Vamos agora olhar o perfil por sexo. Para sexo tanto a solução para 3 quanto para 4 clusters apresentam um uso maior por mulheres. 7.4.3 Estado Civil Para o caso do Estado Civil vemos que pessoas casadas são as que mais usam o serviços em todos os grupos em mandas as soluções. 7.4.4 Tipo de Plano Para o tipo de plano, notamos que o clister formado por pessoas que somente fazem ligações locais não apresentam nenhum tipo de plano. O time de marketing pode investir neste cluster para fazer-los mudar de plano. 7.4.5 Churn Por último temos a variável status que mostra o churn dos clientes, o time de marketing conhecendo melhor o que cada grupo significa podem fazer estratégias baseadas na clusterização para tentar reter os clientes. 7.5 Conclusão Da análise feita nos dados e nos métodos de Clusters hierárquico e não-hierárquico, chegamos na resposta que o número ideal de clusters para ser usado na empresa está entre 3 e 4. Além disso, dos métodos apresentados, utilizar o método Ward de clusterização hierárquica para descobrir os centroides e depois aplicar o método não-hierárquico k-means aparentou ser um método muito poderoso para clusterização. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
